---
title: "Estimation of Daily Water Temperature using Random Forest"
author: "B Steele"
date: today
date-format: long
format: pdf
editor: 
  visual:
    theme: sky
---

```{r env-set-up, echo=FALSE, message=FALSE}
library(tidyverse)
library(reticulate)
library(kableExtra)

# read in temp data
NW_temp <- read_csv('~/OneDrive - Colostate/NASA-Northern/data/waterQuality/harmonized/manual_temperature_data_NW_harmonized_v2023-08-30.csv')
surf_temp <- NW_temp %>% 
  group_by(date, feature) %>% 
  arrange(depth) %>% 
  slice(1) %>% 
  filter(station %in% c('CL-DAM1', 'GR-DAM', 'GL-MID', 'HT-DIX', 
                        'SM-DAM', 'WC-DAM', 'WG-DAM'))
  
NW_estimates <- read_csv('~/OneDrive - Colostate/NASA-Northern/data/remoteSensing/NW_lake_LandsatC2_ST_v2023-05-31.csv') %>% 
  select(date, GNIS_Name, Permanent_Identifier, med_SurfaceTemp) %>% 
  rename(feature = GNIS_Name) %>% 
  mutate(value = med_SurfaceTemp - 273.15,
         feature = case_when(feature == 'Lake Granby' ~ 'Granby Reservoir',
                             feature == 'Carter Lake Reservoir' ~ 'Carter Lake',
                             TRUE ~ feature),
         station = 'sat')

all_NW_temp <- full_join(surf_temp, NW_estimates)

# weather data
weather <- read_csv('~/OneDrive - Colostate/NASA-Northern/data/climate/aggregated/NW_NLDAS_climate_1-3-5d_previous_1984-01-01_2023-05-17_v2023-05-25.csv') %>% 
  rename(feature = lake) %>% 
  pivot_longer(cols = c('tot_precip_mm', 'max_temp_degC', 'mean_temp_degC', 
                        'min_temp_degC', 'tot_sol_rad_Wpm2', 'min_wind_mps',
                        'mean_wind_mps', 'max_wind_mps'),
               names_to = 'variable') %>% 
  pivot_wider(names_from = c('variable', 'n_prev_days'),
              names_sep = '_',
              values_from = 'value') %>% 
  mutate(feature = if_else(feature == 'Lake Granby',
                           'Granby Reservoir',
                           feature))

# static
static <- read_csv('~/OneDrive - Colostate/NASA-Northern/data/static_vars/static_vars_7_lakes.csv')

# join together for full dataset
full_dataset = left_join(static, all_NW_temp) %>% 
  left_join(., weather) %>% 
  select(-c(Permanent_Identifier, med_SurfaceTemp, depth, time, parameter, station)) %>% 
  arrange(date) %>% 
  filter(complete.cases(.))

# drop windy gap for incomplete
full_dataset <- full_dataset %>% 
  filter(feature != 'Windy Gap Reservoir')

# remove carter lake for test set
test = full_dataset %>% 
  filter(feature == 'Carter Lake')

# remove carter from full
train_validate = anti_join(full_dataset, test) 

train_val1 <- train_validate %>% 
  filter(date < '2008-01-01')
train_val2 <- train_validate %>% 
  filter(date >= '2008-01-01', 
         date < '2013-01-01')
train_val3 <- train_validate %>% 
  filter(date >= '2013-01-01', 
         date < '2018-01-01')
train_val4 <- train_validate %>% 
  filter(date >= '2018-01-01', 
         date < '2023-01-01')

# activate conda env
use_condaenv('~/miniconda3/envs/env_ATSML/')

```

```{python, import-modules}
#| echo: false
import sys
import numpy as np
import matplotlib.pyplot as plt

import pandas as pd
import datetime
import sklearn
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import export_graphviz
from sklearn.inspection import permutation_importance
import pydot
import matplotlib.pyplot as plt
```

```{python, label-feature-setup}
#| echo: false

# grab the values we want to predict
labels_1 = np.array(r.train_val1['value'])
labels_2 = np.array(r.train_val2['value'])
labels_3 = np.array(r.train_val3['value'])
labels_4 = np.array(r.train_val4['value'])

# and remove the labels from the dataset containing the feature set
features1 = (r.train_val1
  .drop(['value', 'feature', 'date'], axis = 1))
features2 = (r.train_val2
  .drop(['value', 'feature', 'date'], axis = 1))
features3 = (r.train_val3
  .drop(['value', 'feature', 'date'], axis = 1))
features4 = (r.train_val4
  .drop(['value', 'feature', 'date'], axis = 1))

# Saving feature names for later use
feature_list = list(features1.columns)

# Convert to numpy array
features1 = np.array(features1)
features2 = np.array(features2)
features3 = np.array(features3)
features4 = np.array(features4)

# set random state
rs = 37
```

## Checkpoint Issues:

-   is this a defensible way to do train/val/test? Should I leave an additional lake out for validation?
-   for the train/val that is timeseries aware, why can't I seem to use it?
-   how do I establish a baseline for this? better than yesterday-is-today? (when I don't have a daily timeseries, can I do this?)
-   

## Scientific motivation and problem statement:

Water temperature is often a reliable indicator of general water quality (cite). Active monitoring of lakes, especially those that are difficult to access by monitoring personnel, is difficult. Additionally, manual monitoring of waterbodies (by physically visiting a site) and sensor networks to monitor water temperature, are costly endeavors (cite).

In this example, I will use Random Forest to estimate water surface temperature for reservoirs with long manual monitoring data from Northern Water. The features that I will be using to estimate surface temperature include summary NLDAS meteorological data (air temperature, precipitation, solar radiation, and wind) as well as static values for each of the reservoirs (elevation, surface area, maximum depth, volume, and shoreline distance).

In addition to the manual sampling record that is maintained by Northern water, I will be leveraging surface temperature estimates from the Landsat constellation, Landsat 4-9. These thermal estimates are well-aligned with the manual monitoring data for the 7 reservoirs. 'Surface temperature' in the manual sampling record for this example is any measured temperature at \>= 1m depth. I retain only the top-most value for temperature. Static variables are only available for 6 of 7 reservoirs, so Windy Gap reservoir has been dropped from this analysis.

\[\[add units to table\]\]

```{r static-vars-table, echo = F}
static %>% 
  kbl(format = 'markdown', 
      caption = 'Static variables used in the Random Forest algorithm. Windy Gap
Reservoir has incomplete data and has been dropped from this analysis.')
```

Ideally, implementation of this algorithm will include application to lakes that have only Landsat-derived temperature estimates and that are outside of this dataset. Because I want this algorithm to perform well on new lakes, I want to take steps to make sure that it is not overfit to these specific lakes.

## Training/Validation/Testing

It's clear that there are site-level differences in temperature range and general seasonal response (fig?). These differences are likely due to static variables that differentiate these lakes. That said, if I add in site-level information, the algorithm will quickly learn those key attributes and likely overfit to the data, not allowing for generalization beyond these lakes.

Due to this, my test set will be comprised of data from a lake that was never used in the train/validate set (Carter Lake).

For validation, I'll be using timeseries-aware k-fold training and validation sets, splitting the data in 5-year chunks since 2008, resulting in 4 train-test iterations.

```{python, kfold}
#| echo: false
#| output: false

test = r.test
test = np.array(test)
```

```{python}
# Tunable Parameters for Model
number_of_trees = 40
tree_depth = None 
node_split = 10       # minimum number of training samples needed to split a node
leaf_samples = 10     # minimum number of training samples required to make a leaf node
criterion = 'absolute_error'   


# Instantiate model with number of decision trees prescribed above
# PARAMETERS:
#     n_estimators: number of trees/ensembles
#     random_state: random seed
#     max_depth: maximum depth of each tree
#     criterion: evaluation statistic to split a mode, 'mse'  or 'mae'
#     min_samples_split: minimum number of samples needed to split a node
#     min_samples_leaf: minimum number of samples needed to make a leaf
#     for more, see: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html#sklearn.ensemble.RandomForestRegressor
rf = RandomForestRegressor(n_estimators = number_of_trees, 
                           random_state = rs,
                           min_samples_split = node_split,
                           min_samples_leaf = leaf_samples,
                           criterion = criterion,
                           max_depth = tree_depth)


rf.fit(features1, labels_1)

```

```{python}
# Use the forest's predict method on the test data
predictions = rf.predict(features2)

# Use testing set to validate the performance
# Print out the mean absolute error (MAE)
mae_errors = abs(predictions - labels_2)
#print('Baseline error (MAE): ', round(np.mean(mae_baseline_errors), 2))
print('Error (MAE): ', round(np.mean(mae_errors), 2))

# See its performance (mean squared errors)
mse_errors = np.sqrt(np.mean((predictions - labels_2)**2))
#print('Baseline error (MSE): ', round( mse_baseline_errors, 2))
print('Error (MSE): ', round(mse_errors, 2))
```


## Write up contents:

-   description of any data pre-processing performed and why you did it

-   machine learning setup and reasons for hyperparameter choices when relevant

-   results (e.g. testing accuracy)

-   a detailed discussion of why you don't think you have overfit

-   a detailed discussion of why you think the results are better (or worse if that is the case) than a baseline approach of your choice (e.g. random chance, linear regression, climatology, etc)

-   concluding thoughts including any insights gained from your efforts
