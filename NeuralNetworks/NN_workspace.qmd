---
title: "Neural Networks Workspace"
format: html
editor: visual
jupyter: python3
---

```{r env-set-up}
library(tidyverse)
library(reticulate)
library(kableExtra)

# file paths
is_data = '~/OneDrive - Colostate/NASA-Northern/data/waterQuality/harmonized/'
rs_data = '~/OneDrive - Colostate/NASA-Northern/data/remoteSensing/estimates/'

# activate conda env
use_condaenv('~/miniconda3/envs/env_ATSML/')

```

```{python, import-modules}
#| echo: false
import sys
import numpy as np
import seaborn as sb

import pandas as pd
import datetime
import tensorflow as tf
import sklearn
from sklearn.preprocessing import FunctionTransformer
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.compose import ColumnTransformer

# import pydot
import matplotlib.pyplot as plt
import matplotlib.patches as patches

print(f"python version = {sys.version}")
print(f"numpy version = {np.__version__}")
print(f"tensorflow version = {tf.__version__}")

# set random state
rs = 37
```

## Read in temp data and make train-val-test

```{r raw_data}
# read in temp data
NW_temp <- read_csv(file.path(is_data, 'manual_temperature_data_NW_harmonized_v2023-08-30.csv'))
surf_temp <- NW_temp %>% 
  group_by(date, feature) %>% 
  arrange(depth) %>% 
  slice(1) %>% 
  filter(station %in% c('CL-DAM1', 'GR-DAM', 'GL-MID', 'HT-DIX', 
                        'SM-DAM', 'WC-DAM', 'WG-DAM'))
  
NW_estimates <- read_csv(file.path(rs_data, 'SurfTemp_linearCorrection_v2023-09-28.csv')) %>% 
  rename(feature = GNIS_Name) %>% 
  mutate(value = adj_medTemp,
         feature = case_when(feature == 'Lake Granby' ~ 'Granby Reservoir',
                             feature == 'Carter Lake Reservoir' ~ 'Carter Lake',
                             feature == 'Shadow Mountain Lake' ~ 'Shadow Mountain Reservoir',
                             TRUE ~ feature),
         station = 'sat') %>% 
  filter(location_type == 'poi_center')

all_NW_temp <- full_join(surf_temp, NW_estimates)

# weather data
weather <- read_csv('~/OneDrive - Colostate/NASA-Northern/data/climate/aggregated/NW_NLDAS_climate_1-3-5d_previous_1984-01-01_2023-05-17_v2023-05-25.csv') %>% 
  rename(feature = lake) %>% 
  pivot_longer(cols = c('tot_precip_mm', 'max_temp_degC', 'mean_temp_degC', 
                        'min_temp_degC', 'tot_sol_rad_Wpm2', 'min_wind_mps',
                        'mean_wind_mps', 'max_wind_mps'),
               names_to = 'variable') %>% 
  pivot_wider(names_from = c('variable', 'n_prev_days'),
              names_sep = '_',
              values_from = 'value') %>% 
  mutate(feature = if_else(feature == 'Lake Granby',
                           'Granby Reservoir',
                           feature))
# static
static <- read_csv('~/OneDrive - Colostate/NASA-Northern/data/static_vars/static_vars_7_lakes.csv')

# join together for full dataset
full_dataset = left_join(static, all_NW_temp) %>% 
  left_join(., weather) %>% 
  filter(between(month, 4, 10)) %>% 
  select(-c(med_SurfaceTemp, adj_medTemp, depth, time, parameter, month,
            station, Latitude, Longitude, lakeID, HarmonizedName, 
            location_type, mission)) %>% 
  arrange(date) %>% 
  filter(complete.cases(.)) 

# drop windy gap for incomplete
full_dataset <- full_dataset %>% 
  filter(feature != 'Windy Gap Reservoir') %>% 
  mutate(day_of_year = yday(date))

```

## Make train-val-test sets

Test data will be from 2021 forward (2y of data)

```{r tst}
test <- full_dataset %>% 
  filter(date >= ymd('2021-01-01'))

```

### make a few iterations of train-val

One will be leave-one-out by lake, the other splitting by timeseries in \~10y increments

```{r training_set}
#full training set
training <- anti_join(full_dataset, test) %>% 
  mutate(across(c("tot_precip_mm_1", "tot_precip_mm_3", "tot_precip_mm_5"),
                ~ . + 0.0001))
```

#### Look at the training data

```{r}
make_histogram <- function(param){
  param_sym = sym(param)
  ggplot(training, aes(x = !!{{ param_sym }})) +
    geom_histogram() +
    theme_bw()
}

param_list = names(training)
param_list = param_list[8:length(param_list)]

map(param_list, make_histogram)
```

Precip data needs log transformation before standardization due to histo frequency.

Wind data needs sqrt transormation.

```{python make-pre-processing-pipeline}
log_transformer = FunctionTransformer(np.log, 
  inverse_func=np.exp, 
  feature_names_out = "one-to-one")
sqrt_transformer = FunctionTransformer(np.sqrt, 
  inverse_func=np.square,
  feature_names_out = "one-to-one")

precip_pipeline = make_pipeline(
  log_transformer,
  StandardScaler()
)

wind_pipeline = make_pipeline(
  sqrt_transformer,
  StandardScaler()
)

precip_vars = ["tot_precip_mm_1", "tot_precip_mm_3", "tot_precip_mm_5"]

wind_vars = ["min_wind_mps_1", "mean_wind_mps_1", "max_wind_mps_1",
  "min_wind_mps_3", "mean_wind_mps_3", "max_wind_mps_3",
  "min_wind_mps_5", "mean_wind_mps_5", "max_wind_mps_5"]

std_vars = ["value", "day_of_year",
  "max_temp_degC_1", "mean_temp_degC_1", "min_temp_degC_1", "tot_sol_rad_Wpm2_1",
  "max_temp_degC_3", "mean_temp_degC_3", "min_temp_degC_3", "tot_sol_rad_Wpm2_3", 
  "max_temp_degC_5", "mean_temp_degC_5", "min_temp_degC_5", "tot_sol_rad_Wpm2_5"]

preprocessing = ColumnTransformer([
  ("precip", precip_pipeline, precip_vars),
  ("wind", wind_pipeline, wind_vars),
  ("std", StandardScaler(), std_vars)
])
```

```{r make-train-val-sets}
# using a leave-one-out method for train/validate
train1 <- training %>% 
  filter(feature != 'Grand Lake')
val1 = anti_join(training, train1)
train2 <- training %>% 
  filter(feature != 'Horsetooth Reservoir')
val2 = anti_join(training, train2)
train3 <- training %>% 
  filter(feature != 'Shadow Mountain Reservoir')
val3 = anti_join(training, train3)
train4 <- training %>% 
  filter(feature != 'Granby Reservoir')
val4 = anti_join(training, train4)
train5 <- training %>% 
  filter(feature != 'Carter Lake')
val5 = anti_join(training, train5)
train6 <- training %>% 
  filter(feature != 'Willow Creek Reservoir')
val6 = anti_join(training, train6)

# using time blocks
train1_ts <- training %>% 
  filter(!between(date, ymd('1984-01-01'), ymd('1995-01-01')))
val1_ts = anti_join(training, train1_ts)
train2_ts <- training %>% 
  filter(!between(date, ymd('1995-01-01'), ymd('2005-01-01')))
val2_ts = anti_join(training, train2_ts)
train3_ts <- training %>% 
  filter(!between(date, ymd('2005-01-01'), ymd('2015-01-01')))
val3_ts = anti_join(training, train3_ts)
train4_ts <- training %>% 
  filter(!between(date, ymd('2015-01-01'), ymd('2021-01-01')))
val4_ts = anti_join(training, train4_ts)
```

### Bring data over to py and apply preprocessing pipeline

```{python, train-val-to-py}
#| echo: false
#bring data over from R
train1 = r.train1
train2 = r.train2
train3 = r.train3
train4 = r.train4
train5 = r.train5
train6 = r.train6
val1 = r.val1
val2 = r.val2
val3 = r.val3
val4 = r.val4
val5 = r.val5
val6 = r.val6

train1_ts = r.train1_ts
train2_ts = r.train2_ts
train3_ts = r.train3_ts
train4_ts = r.train4_ts

val1_ts = r.val1_ts
val2_ts = r.val2_ts
val3_ts = r.val3_ts
val4_ts = r.val4_ts
```

#### Format LOO dataset for algo dev

```{python labels-features}
# grab the values we want to predict
labels_1 = np.array(train1['value'])
labels_2 = np.array(train2['value'])
labels_3 = np.array(train3['value'])
labels_4 = np.array(train4['value'])
labels_5 = np.array(train5['value'])
labels_6 = np.array(train6['value'])

# grab the values we want to predict
val_labels_1 = np.array(val1['value'])
val_labels_2 = np.array(val2['value'])
val_labels_3 = np.array(val3['value'])
val_labels_4 = np.array(val4['value'])
val_labels_5 = np.array(val5['value'])
val_labels_6 = np.array(val6['value'])

# and remove the labels from the dataset containing the feature set
features1 = (r.train1
  .drop(['value', 'feature', 'date'], axis = 1))
features2 = (r.train2
  .drop(['value', 'feature', 'date'], axis = 1))
features3 = (r.train3
  .drop(['value', 'feature', 'date'], axis = 1))
features4 = (r.train4
  .drop(['value', 'feature', 'date'], axis = 1))
features5 = (r.train5
  .drop(['value', 'feature', 'date'], axis = 1))
features6 = (r.train6
  .drop(['value', 'feature', 'date'], axis = 1))

# and remove the labels from the dataset containing the feature set
val_features1 = (r.val1
  .drop(['value', 'feature', 'date'], axis = 1))
val_features2 = (r.val2
  .drop(['value', 'feature', 'date'], axis = 1))
val_features3 = (r.val3
  .drop(['value', 'feature', 'date'], axis = 1))
val_features4 = (r.val4
  .drop(['value', 'feature', 'date'], axis = 1))
val_features5 = (r.val5
  .drop(['value', 'feature', 'date'], axis = 1))
val_features6 = (r.val6
  .drop(['value', 'feature', 'date'], axis = 1))

# Saving feature names for later use
feature_list = list(features1.columns)

# Convert to numpy array
features1 = np.array(features1)
features2 = np.array(features2)
features3 = np.array(features3)
features4 = np.array(features4)
features5 = np.array(features5)
features6 = np.array(features6)

# Convert to numpy array
val_features1 = np.array(val_features1)
val_features2 = np.array(val_features2)
val_features3 = np.array(val_features3)
val_features4 = np.array(val_features4)
val_features5 = np.array(val_features5)
val_features6 = np.array(val_features6)

```

#### And for ts chunks

```{python timeseries-test-val}
#| echo: false

# grab the values we want to predict
labels_1_ts = np.array(train1_ts['value'])
labels_2_ts = np.array(train2_ts['value'])
labels_3_ts = np.array(train3_ts['value'])
labels_4_ts = np.array(train4_ts['value'])

# grab the values we want to predict
val_labels_1_ts = np.array(val1_ts['value'])
val_labels_2_ts = np.array(val2_ts['value'])
val_labels_3_ts = np.array(val3_ts['value'])
val_labels_4_ts = np.array(val4_ts['value'])

# and remove the labels from the dataset containing the feature set
features1_ts = (train1_ts
  .drop(['value', 'feature', 'date'], axis = 1))
features2_ts = (train2_ts
  .drop(['value', 'feature', 'date'], axis = 1))
features3_ts = (train3_ts
  .drop(['value', 'feature', 'date'], axis = 1))
features4_ts = (train4_ts
  .drop(['value', 'feature', 'date'], axis = 1))

# and remove the labels from the dataset containing the feature set
val_features1_ts = (val1_ts
  .drop(['value', 'feature', 'date'], axis = 1))
val_features2_ts = (val2_ts
  .drop(['value', 'feature', 'date'], axis = 1))
val_features3_ts = (val3_ts
  .drop(['value', 'feature', 'date'], axis = 1))
val_features4_ts = (val4_ts
  .drop(['value', 'feature', 'date'], axis = 1))

# Convert to numpy array
features1_ts = np.array(features1_ts)
features2_ts = np.array(features2_ts)
features3_ts = np.array(features3_ts)
features4_ts = np.array(features4_ts)

# Convert to numpy array
val_features1_ts = np.array(val_features1_ts)
val_features2_ts = np.array(val_features2_ts)
val_features3_ts = np.array(val_features3_ts)
val_features4_ts = np.array(val_features4_ts)

```

### Make a baseline dataset

```{r baseline}
baseline_by_date <- full_dataset %>% 
  group_by(day_of_year) %>% 
  summarize(mean_temp_by_date_deg_C = mean(value),
            n = n()) %>% 
  left_join(full_dataset, .) %>% 
  #remove days where there are less than or equal to 3 observations contributing to the mean
  filter(n > 3)

```

#### Calculate baseline error

```{python, baseline}
#| echo: false
baseline_day = r.baseline_by_date
mae_baseline_day_errors = np.mean(abs(baseline_day['value'] - baseline_day['mean_temp_by_date_deg_C']))
baseline_mae_err_text = round(mae_baseline_day_errors, 2)

mse_baseline_day_errors = np.sqrt(np.mean(abs(baseline_day['value'] - baseline_day['mean_temp_by_date_deg_C'])**2))
baseline_mse_err_text = round(mse_baseline_day_errors, 2)
```

## 

```{python, preproc-loo}
#| echo: false
def preproc_train(data):
  pp = preprocessing.fit_transform(data)
  df = pd.DataFrame(
    pp, 
    columns = preprocessing.get_feature_names_out(),
    index = data.index
  )
  return df

train1_df = preproc_df(train1)
train2_df = preproc_df(train2)
train3_df = preproc_df(train3)
train4_df = preproc_df(train4)
train5_df = preproc_df(train5)
train6_df = preproc_df(train6)
```
