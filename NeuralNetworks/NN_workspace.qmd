---
title: "Estimation of Daily Water Temperature using Dense Neural Networks"
author: "B Steele"
date: today
date-format: long
format: pdf
editor: 
  visual:
    theme: sky
---

```{r, env-set-up}
library(tidyverse)
library(reticulate)
library(kableExtra)

# file paths
nn_dir = '~/OneDrive - Colostate/NASA-Northern/data/NN_train_val_test/'

# activate conda env
use_condaenv('~/miniconda3/envs/env_ATSML/')

```

```{python, import-modules}
#| echo: false

#high level modules
import os
import sys
import imp
import numpy as np
import seaborn as sb
import pandas as pd
import datetime
import pickle

# ml/ai modules
import tensorflow as tf
# Let's import some different things we will use to build the neural network
from tensorflow.keras import Model
from tensorflow.keras.layers import Dense, Input, Dropout, Softmax

# import plt
import matplotlib.pyplot as plt
import matplotlib.patches as patches

# import custom modules
imp.load_source("settings","NeuralNetworks/settings.py")
from settings import settings
imp.load_source("tvt", "NeuralNetworks/preprocessing.py")
from tvt import train1, val1, train2, val2, train3, val3, train4, val4, train5, val5, train6, val6
from tvt import train1_ts, val1_ts, train2_ts, val2_ts, train3_ts, val3_ts, train4_ts, val4_ts
imp.load_source("architecture", "NeuralNetworks/architecture.py")
from architecture import build_model, compile_model

print(f"python version = {sys.version}")
print(f"numpy version = {np.__version__}")
print(f"tensorflow version = {tf.__version__}")
```

### Look at the training data

```{r}
make_histogram <- function(param){
  param_sym = sym(param)
  ggplot(training, aes(x = !!{{ param_sym }})) +
    geom_histogram() +
    theme_bw()
}

training <- read_csv(file.path(nn_dir, "training_set_up_to_2021_v2023-11-08.csv"))
param_list = names(training)
param_list = param_list[8:length(param_list)]

map(param_list, make_histogram)
```

Precip data needs log transformation before standardization due to histo frequency.

Wind data needs sqrt transormation.

These transformations have been made in the _preprocessing.py script.

## Train/Val sets

### Format LOO dataset for algo dev

```{python, labels-features}
#| echo: false

# grab the values we want to predict
labels_1 = np.array(train1['value'])
labels_2 = np.array(train2['value'])
labels_3 = np.array(train3['value'])
labels_4 = np.array(train4['value'])
labels_5 = np.array(train5['value'])
labels_6 = np.array(train6['value'])

# grab the values we want to predict
val_labels_1 = np.array(val1['value'])
val_labels_2 = np.array(val2['value'])
val_labels_3 = np.array(val3['value'])
val_labels_4 = np.array(val4['value'])
val_labels_5 = np.array(val5['value'])
val_labels_6 = np.array(val6['value'])

# and remove the labels from the dataset containing the feature set
features1 = (train1
  .drop(['value', 'feature', 'date'], axis = 1))
features2 = (train2
  .drop(['value', 'feature', 'date'], axis = 1))
features3 = (train3
  .drop(['value', 'feature', 'date'], axis = 1))
features4 = (train4
  .drop(['value', 'feature', 'date'], axis = 1))
features5 = (train5
  .drop(['value', 'feature', 'date'], axis = 1))
features6 = (train6
  .drop(['value', 'feature', 'date'], axis = 1))

# and remove the labels from the dataset containing the feature set
val_features1 = (val1
  .drop(['value', 'feature', 'date'], axis = 1))
val_features2 = (val2
  .drop(['value', 'feature', 'date'], axis = 1))
val_features3 = (val3
  .drop(['value', 'feature', 'date'], axis = 1))
val_features4 = (val4
  .drop(['value', 'feature', 'date'], axis = 1))
val_features5 = (val5
  .drop(['value', 'feature', 'date'], axis = 1))
val_features6 = (val6
  .drop(['value', 'feature', 'date'], axis = 1))

# Saving feature names for later use
feature_list = list(features1.columns)

# Convert to numpy array
features1 = np.array(features1)
features2 = np.array(features2)
features3 = np.array(features3)
features4 = np.array(features4)
features5 = np.array(features5)
features6 = np.array(features6)

# Convert to numpy array
val_features1 = np.array(val_features1)
val_features2 = np.array(val_features2)
val_features3 = np.array(val_features3)
val_features4 = np.array(val_features4)
val_features5 = np.array(val_features5)
val_features6 = np.array(val_features6)

```

#### And for ts chunks

```{python, timeseries-test-val}
#| echo: false

# grab the values we want to predict
labels_1_ts = np.array(train1_ts['value'])
labels_2_ts = np.array(train2_ts['value'])
labels_3_ts = np.array(train3_ts['value'])
labels_4_ts = np.array(train4_ts['value'])

# grab the values we want to predict
val_labels_1_ts = np.array(val1_ts['value'])
val_labels_2_ts = np.array(val2_ts['value'])
val_labels_3_ts = np.array(val3_ts['value'])
val_labels_4_ts = np.array(val4_ts['value'])

# and remove the labels from the dataset containing the feature set
features1_ts = (train1_ts
  .drop(['value', 'feature', 'date'], axis = 1))
features2_ts = (train2_ts
  .drop(['value', 'feature', 'date'], axis = 1))
features3_ts = (train3_ts
  .drop(['value', 'feature', 'date'], axis = 1))
features4_ts = (train4_ts
  .drop(['value', 'feature', 'date'], axis = 1))

# and remove the labels from the dataset containing the feature set
val_features1_ts = (val1_ts
  .drop(['value', 'feature', 'date'], axis = 1))
val_features2_ts = (val2_ts
  .drop(['value', 'feature', 'date'], axis = 1))
val_features3_ts = (val3_ts
  .drop(['value', 'feature', 'date'], axis = 1))
val_features4_ts = (val4_ts
  .drop(['value', 'feature', 'date'], axis = 1))

# Convert to numpy array
features1_ts = np.array(features1_ts)
features2_ts = np.array(features2_ts)
features3_ts = np.array(features3_ts)
features4_ts = np.array(features4_ts)

# Convert to numpy array
val_features1_ts = np.array(val_features1_ts)
val_features2_ts = np.array(val_features2_ts)
val_features3_ts = np.array(val_features3_ts)
val_features4_ts = np.array(val_features4_ts)

```

## Make a baseline dataset

```{r, baseline}
baseline_by_date <- training %>% 
  group_by(day_of_year) %>% 
  summarize(mean_temp_by_date_deg_C = mean(value),
            n = n()) %>% 
  left_join(training, .) %>% 
  #remove days where there are less than or equal to 3 observations contributing to the mean
  filter(n > 3)
```

### Calculate baseline error

```{python, baseline-error}
#| echo: false
baseline_day = r.baseline_by_date
mae_baseline_day_errors = np.mean(abs(baseline_day['value'] - baseline_day['mean_temp_by_date_deg_C']))
mse_baseline_day_errors = np.sqrt(np.mean(abs(baseline_day['value'] - baseline_day['mean_temp_by_date_deg_C'])**2))
```

## Make NN architecture

```{python, hyperparm-settings-train}

tf.keras.backend.clear_session()
tf.keras.utils.set_random_seed(settings["basic"]["random_seed"])

# define the early stopping callback
early_stopping_callback = tf.keras.callbacks.EarlyStopping(
  monitor="val_loss", 
  patience=settings["basic"]["patience"], 
  restore_best_weights=True, 
  mode="auto"
)

```

```{python}
## LOO 1
model_1 = build_model(
  features1, 
  labels_1, 
  settings["basic"])

model_1 = compile_model(
  model_1, 
  settings['basic'])

# train the model via model.fit
history_1 = model_1.fit(
  features1, 
  labels_1, 
  epochs=settings['basic']["max_epochs"],
  batch_size=settings['basic']["batch_size"],
  shuffle=True,
  validation_data=[val_features1, val_labels_1],
  callbacks=[early_stopping_callback],
  verbose=1,
)

## LOO 2
model_2 = build_model(
  features2,
  labels_2, 
  settings["basic"])
model_2 = compile_model(model_2, settings["basic"])

# train the model via model.fit
history_2 = model_2.fit(
  features2,
  labels_2,
  epochs=settings["basic"]["max_epochs"],
  batch_size=settings["basic"]["batch_size"],
  shuffle=True,
  validation_data=[val_features2, val_labels_2],
  callbacks=[early_stopping_callback],
  verbose=1,
)

## LOO 3

model_3 = build_model(
  features3,
  labels_3,
  settings["basic"])
model_3 = compile_model(model_3, settings["basic"])

# train the model via model.fit
history_3 = model_3.fit(
  features3,
  labels_3,
  epochs=settings["basic"]["max_epochs"],
  batch_size=settings["basic"]["batch_size"],
  shuffle=True,
  validation_data=[val_features3, val_labels_3],
  callbacks=[early_stopping_callback],
  verbose=1,
)

## LOO 4

model_4 = build_model(
  features4,
  labels_4,
  settings["basic"])
model_4 = compile_model(model_4, settings["basic"])

# train the model via model.fit
history_4 = model_4.fit(
  features4,
  labels_4,
  epochs=settings["basic"]["max_epochs"],
  batch_size=settings["basic"]["batch_size"],
  shuffle=True,
  validation_data=[val_features4, val_labels_4],
  callbacks=[early_stopping_callback],
  verbose=1,
)

## LOO 5

model_5 = build_model(
  features5,
  labels_5,
  settings["basic"])
model_5 = compile_model(model_5, settings["basic"])

# train the model via model.fit
history_5 = model_5.fit(
  features5,
  labels_5,
  epochs=settings["basic"]["max_epochs"],
  batch_size=settings["basic"]["batch_size"],
  shuffle=True,
  validation_data=[val_features5, val_labels_5],
  callbacks=[early_stopping_callback],
  verbose=1,
)

## LOO 6

model_6 = build_model(
  features6,
  labels_6,
  settings["basic"])
model_6 = compile_model(model_6, settings["basic"])

# train the model via model.fit
history_6 = model_6.fit(
  features6,
  labels_6,
  epochs=settings["basic"]["max_epochs"],
  batch_size=settings["basic"]["batch_size"],
  shuffle=True,
  validation_data=[val_features6, val_labels_6],
  callbacks=[early_stopping_callback],
  verbose=1,
)
```

```{python, train-1}
"fig, axs = plt.subplots(1, 2, figsize=(10, 4))

axs[0].plot(history_1.history["loss"], label="training")
axs[0].plot(history_1.history["val_loss"], label="validation")
axs[0].set_xlabel("Epoch")
axs[0].set_ylabel("Loss")
axs[0].legend()

axs[1].plot(history_1.history["loss"], label="training")
axs[1].plot(history_1.history["val_loss"], label="validation")
axs[1].set_xlabel("Epoch")
axs[1].set_ylabel("Mean Squared Error")
axs[1].legend()

fig.suptitle("LOO dataset 1")

fig.show()
```


```{python, train-2}
fig, axs = plt.subplots(1, 2, figsize=(10, 4))

axs[0].plot(history_2.history["loss"], label="training")
axs[0].plot(history_2.history["val_loss"], label="validation")
axs[0].set_xlabel("Epoch")
axs[0].set_ylabel("Loss")
axs[0].legend()

axs[1].plot(history_2.history["loss"], label="training")
axs[1].plot(history_2.history["val_loss"], label="validation")
axs[1].set_xlabel("Epoch")
axs[1].set_ylabel("Mean Squared Error")
axs[1].legend()

fig.suptitle("LOO dataset 2")

fig.show()
```

```{python, train-3}
fig, axs = plt.subplots(1, 2, figsize=(10, 4))

axs[0].plot(history_3.history["loss"], label="training")
axs[0].plot(history_3.history["val_loss"], label="validation")
axs[0].set_xlabel("Epoch")
axs[0].set_ylabel("Loss")
axs[0].legend()

axs[1].plot(history_3.history["loss"], label="training")
axs[1].plot(history_3.history["val_loss"], label="validation")
axs[1].set_xlabel("Epoch")
axs[1].set_ylabel("Mean Squared Error")
axs[1].legend()

fig.suptitle("LOO dataset 3")

fig.show()
```

```{python, train-4}
fig, axs = plt.subplots(1, 2, figsize=(10, 4))

axs[0].plot(history_4.history["loss"], label="training")
axs[0].plot(history_4.history["val_loss"], label="validation")
axs[0].set_xlabel("Epoch")
axs[0].set_ylabel("Loss")
axs[0].legend()

axs[1].plot(history_4.history["loss"], label="training")
axs[1].plot(history_4.history["val_loss"], label="validation")
axs[1].set_xlabel("Epoch")
axs[1].set_ylabel("Mean Squared Error")
axs[1].legend()

fig.suptitle("LOO dataset 4")

fig.show()
```

```{python, train-5}
fig, axs = plt.subplots(1, 2, figsize=(10, 4))

axs[0].plot(history_5.history["loss"], label="training")
axs[0].plot(history_5.history["val_loss"], label="validation")
axs[0].set_xlabel("Epoch")
axs[0].set_ylabel("Loss")
axs[0].legend()

axs[1].plot(history_5.history["loss"], label="training")
axs[1].plot(history_5.history["val_loss"], label="validation")
axs[1].set_xlabel("Epoch")
axs[1].set_ylabel("Mean Squared Error")
axs[1].legend()

fig.suptitle("LOO dataset 5")

fig.show()
```

```{python, train-6}
fig, axs = plt.subplots(1, 2, figsize=(10, 4))

axs[0].plot(history_6.history["loss"], label="training")
axs[0].plot(history_6.history["val_loss"], label="validation")
axs[0].set_xlabel("Epoch")
axs[0].set_ylabel("Loss")
axs[0].legend()

axs[1].plot(history_6.history["loss"], label="training")
axs[1].plot(history_6.history["val_loss"], label="validation")
axs[1].set_xlabel("Epoch")
axs[1].set_ylabel("Mean Squared Error")
axs[1].legend()

fig.suptitle("LOO dataset 6")

fig.show()
```