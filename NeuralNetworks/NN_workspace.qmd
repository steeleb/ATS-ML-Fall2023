---
title: "Estimation of Daily Water Temperature using Dense Neural Networks"
author: "B Steele"
date: today
date-format: long
format: pdf
editor: 
  visual:
    theme: sky
---

```{r, env-set-up}
library(tidyverse)
library(reticulate)
library(kableExtra)

# file paths
nn_dir = '~/OneDrive - Colostate/NASA-Northern/data/NN_train_val_test/'

# activate conda env
use_condaenv('~/miniconda3/envs/env_ATSML/')

```

```{python, import-modules}
#| echo: false

#high level modules
import sys
import imp
import numpy as np
import seaborn as sb
import pandas as pd
import datetime
import pickle

# ml/ai modules
import tensorflow as tf
# Let's import some different things we will use to build the neural network
from tensorflow.keras import Model
from tensorflow.keras.layers import Dense, Input, Dropout, Softmax

# import plt
import matplotlib.pyplot as plt
import matplotlib.patches as patches

# import custom modules
imp.load_source("settings","NeuralNetworks/settings.py")
from settings import settings

print(f"python version = {sys.version}")
print(f"numpy version = {np.__version__}")
print(f"tensorflow version = {tf.__version__}")

# set random state
rs = 37
```

### Look at the training data

```{r}
make_histogram <- function(param){
  param_sym = sym(param)
  ggplot(training, aes(x = !!{{ param_sym }})) +
    geom_histogram() +
    theme_bw()
}

training <- read_csv(file.path(nn_dir, "training_set_up_to_2021_v2023-11-08.csv"))
param_list = names(training)
param_list = param_list[8:length(param_list)]

map(param_list, make_histogram)
```

Precip data needs log transformation before standardization due to histo frequency.

Wind data needs sqrt transormation.

These transformations have been made in the _preprocessing.py script.

## Train/Val sets

Read in the train/val datasets that were saved as pickle files in _preprocessing.py

```{python, load-train-val}

```

### Format LOO dataset for algo dev

```{python, labels-features}
# grab the values we want to predict
labels_1 = np.array(train1['value'])
labels_2 = np.array(train2['value'])
labels_3 = np.array(train3['value'])
labels_4 = np.array(train4['value'])
labels_5 = np.array(train5['value'])
labels_6 = np.array(train6['value'])

# grab the values we want to predict
val_labels_1 = np.array(val1['value'])
val_labels_2 = np.array(val2['value'])
val_labels_3 = np.array(val3['value'])
val_labels_4 = np.array(val4['value'])
val_labels_5 = np.array(val5['value'])
val_labels_6 = np.array(val6['value'])

# and remove the labels from the dataset containing the feature set
features1 = (train1
  .drop(['value', 'feature', 'date'], axis = 1))
features2 = (train2
  .drop(['value', 'feature', 'date'], axis = 1))
features3 = (train3
  .drop(['value', 'feature', 'date'], axis = 1))
features4 = (train4
  .drop(['value', 'feature', 'date'], axis = 1))
features5 = (train5
  .drop(['value', 'feature', 'date'], axis = 1))
features6 = (train6
  .drop(['value', 'feature', 'date'], axis = 1))

# and remove the labels from the dataset containing the feature set
val_features1 = (val1
  .drop(['value', 'feature', 'date'], axis = 1))
val_features2 = (val2
  .drop(['value', 'feature', 'date'], axis = 1))
val_features3 = (val3
  .drop(['value', 'feature', 'date'], axis = 1))
val_features4 = (val4
  .drop(['value', 'feature', 'date'], axis = 1))
val_features5 = (val5
  .drop(['value', 'feature', 'date'], axis = 1))
val_features6 = (val6
  .drop(['value', 'feature', 'date'], axis = 1))

# Saving feature names for later use
feature_list = list(features1.columns)

# Convert to numpy array
features1 = np.array(features1)
features2 = np.array(features2)
features3 = np.array(features3)
features4 = np.array(features4)
features5 = np.array(features5)
features6 = np.array(features6)

# Convert to numpy array
val_features1 = np.array(val_features1)
val_features2 = np.array(val_features2)
val_features3 = np.array(val_features3)
val_features4 = np.array(val_features4)
val_features5 = np.array(val_features5)
val_features6 = np.array(val_features6)

```

#### And for ts chunks

```{python, timeseries-test-val}
#| echo: false

# grab the values we want to predict
labels_1_ts = np.array(train1_ts['value'])
labels_2_ts = np.array(train2_ts['value'])
labels_3_ts = np.array(train3_ts['value'])
labels_4_ts = np.array(train4_ts['value'])

# grab the values we want to predict
val_labels_1_ts = np.array(val1_ts['value'])
val_labels_2_ts = np.array(val2_ts['value'])
val_labels_3_ts = np.array(val3_ts['value'])
val_labels_4_ts = np.array(val4_ts['value'])

# and remove the labels from the dataset containing the feature set
features1_ts = (train1_ts
  .drop(['value', 'feature', 'date'], axis = 1))
features2_ts = (train2_ts
  .drop(['value', 'feature', 'date'], axis = 1))
features3_ts = (train3_ts
  .drop(['value', 'feature', 'date'], axis = 1))
features4_ts = (train4_ts
  .drop(['value', 'feature', 'date'], axis = 1))

# and remove the labels from the dataset containing the feature set
val_features1_ts = (val1_ts
  .drop(['value', 'feature', 'date'], axis = 1))
val_features2_ts = (val2_ts
  .drop(['value', 'feature', 'date'], axis = 1))
val_features3_ts = (val3_ts
  .drop(['value', 'feature', 'date'], axis = 1))
val_features4_ts = (val4_ts
  .drop(['value', 'feature', 'date'], axis = 1))

# Convert to numpy array
features1_ts = np.array(features1_ts)
features2_ts = np.array(features2_ts)
features3_ts = np.array(features3_ts)
features4_ts = np.array(features4_ts)

# Convert to numpy array
val_features1_ts = np.array(val_features1_ts)
val_features2_ts = np.array(val_features2_ts)
val_features3_ts = np.array(val_features3_ts)
val_features4_ts = np.array(val_features4_ts)

```

## Make a baseline dataset

```{r, baseline}
baseline_by_date <- training %>% 
  group_by(day_of_year) %>% 
  summarize(mean_temp_by_date_deg_C = mean(value),
            n = n()) %>% 
  left_join(training, .) %>% 
  #remove days where there are less than or equal to 3 observations contributing to the mean
  filter(n > 3)

```

### Calculate baseline error

```{python, baseline-error}
#| echo: false
baseline_day = r.baseline_by_date
mae_baseline_day_errors = np.mean(abs(baseline_day['value'] - baseline_day['mean_temp_by_date_deg_C']))
mse_baseline_day_errors = np.sqrt(np.mean(abs(baseline_day['value'] - baseline_day['mean_temp_by_date_deg_C'])**2))
```

## Make NN architecture

```{python, build-compile-model}
#| echo: false
def build_model(x_train, y_train, settings):
  # create input layer
  input_layer = tf.keras.layers.Input(shape=x_train.shape[1:])
  
  # # create hidden layers each with specific number of nodes
  # assert len(settings["hiddens"]) == len(
  #   settings["activations"]
  # ), "hiddens and activations settings must be the same length."
  
  # add dropout layer
  layers = tf.keras.layers.Dropout(rate=settings["dropout_rate"])(input_layer)
  
  for hidden, activation in zip(settings["hiddens"], settings["activations"]):
    layers = tf.keras.layers.Dense(
      units=hidden,
      activation=activation,
      use_bias=True,
      kernel_regularizer=tf.keras.regularizers.l1_l2(l1=0, l2=0),
      bias_initializer=tf.keras.initializers.RandomNormal(seed=settings["random_seed"]),
      kernel_initializer=tf.keras.initializers.RandomNormal(seed=settings["random_seed"]),
      )(layers)
  
  # create output layer
  output_layer = tf.keras.layers.Dense(
    units=1,
    activation="linear",
    use_bias=True,
    bias_initializer=tf.keras.initializers.RandomNormal(seed=settings["random_seed"] + 1),
    kernel_initializer=tf.keras.initializers.RandomNormal(seed=settings["random_seed"] + 2),
  )(layers)
  
  # construct the model
  model = tf.keras.Model(inputs=input_layer, outputs=output_layer)
  model.summary()
  
  return model


def compile_model(model, settings):
  model.compile(
      optimizer=tf.keras.optimizers.Adam(
        learning_rate=settings["learning_rate"],
        ),
      loss=tf.keras.losses.MeanSquaredError()
  )
  return model
```

```{python, hyperparm-settings-train}

tf.keras.backend.clear_session()
tf.keras.utils.set_random_seed(settings["random_seed"])

# define the early stopping callback
early_stopping_callback = tf.keras.callbacks.EarlyStopping(
  monitor="val_loss", 
  patience=settings["patience"], 
  restore_best_weights=True, 
  mode="auto"
)

```

```{python}
model_1 = build_model(
  train1_df.drop('value', axis = 1), 
  train1_df['value'], 
  settings)
model_1 = compile_model(model_1, settings)

# train the model via model.fit
history_1 = model_1.fit(
  train1_df.drop('value', axis = 1), 
  train1_df['value'], 
  epochs=settings["max_epochs"],
  batch_size=settings["batch_size"],
  shuffle=True,
  validation_data=[val1_df.drop('value', axis = 1), val1_df['value']],
  callbacks=[early_stopping_callback],
  verbose=1,
)

model_2 = build_model(
  train2_df.drop('value', axis = 1), 
  train2_df['value'], 
  settings)
model_2 = compile_model(model_2, settings)

# train the model via model.fit
history_2 = model_2.fit(
  train2_df.drop('value', axis = 1), 
  train2_df['value'], 
  epochs=settings["max_epochs"],
  batch_size=settings["batch_size"],
  shuffle=True,
  validation_data=[val2_df.drop('value', axis = 1), val2_df['value']],
  callbacks=[early_stopping_callback],
  verbose=1,
)

model_3 = build_model(
  train3_df.drop('value', axis = 1), 
  train3_df['value'], 
  settings)
model_3 = compile_model(model_3, settings)

# train the model via model.fit
history_3 = model_3.fit(
  train3_df.drop('value', axis = 1), 
  train3_df['value'], 
  epochs=settings["max_epochs"],
  batch_size=settings["batch_size"],
  shuffle=True,
  validation_data=[val3_df.drop('value', axis = 1), val3_df['value']],
  callbacks=[early_stopping_callback],
  verbose=1,
)

model_4 = build_model(
  train4_df.drop('value', axis = 1), 
  train4_df['value'], 
  settings)
model_4 = compile_model(model_4, settings)

# train the model via model.fit
history_4 = model_4.fit(
  train4_df.drop('value', axis = 1), 
  train4_df['value'], 
  epochs=settings["max_epochs"],
  batch_size=settings["batch_size"],
  shuffle=True,
  validation_data=[val4_df.drop('value', axis = 1), val4_df['value']],
  callbacks=[early_stopping_callback],
  verbose=1,
)

model_5 = build_model(
  train5_df.drop('value', axis = 1), 
  train5_df['value'], 
  settings)
model_5 = compile_model(model_5, settings)

# train the model via model.fit
history_5 = model_5.fit(
  train5_df.drop('value', axis = 1), 
  train5_df['value'], 
  epochs=settings["max_epochs"],
  batch_size=settings["batch_size"],
  shuffle=True,
  validation_data=[val5_df.drop('value', axis = 1), val5_df['value']],
  callbacks=[early_stopping_callback],
  verbose=1,
)

model_6 = build_model(
  train6_df.drop('value', axis = 1), 
  train6_df['value'], 
  settings)
model_6 = compile_model(model_6, settings)

# train the model via model.fit
history_6 = model_6.fit(
  train6_df.drop('value', axis = 1), 
  train6_df['value'], 
  epochs=settings["max_epochs"],
  batch_size=settings["batch_size"],
  shuffle=True,
  validation_data=[val6_df.drop('value', axis = 1), val6_df['value']],
  callbacks=[early_stopping_callback],
  verbose=1,
)
```

```{python, train-1}
fig, axs = plt.subplots(1, 2, figsize=(10, 4))

axs[0].plot(history_1.history["loss"], label="training")
axs[0].plot(history_1.history["val_loss"], label="validation")
axs[0].set_xlabel("Epoch")
axs[0].set_ylabel("Loss")
axs[0].legend()

axs[1].plot(history_1.history["loss"], label="training")
axs[1].plot(history_1.history["val_loss"], label="validation")
axs[1].set_xlabel("Epoch")
axs[1].set_ylabel("Mean Squared Error")
axs[1].legend()

fig.suptitle("LOO dataset 1")

fig.show()
```


```{python, train-2}
fig, axs = plt.subplots(1, 2, figsize=(10, 4))

axs[0].plot(history_2.history["loss"], label="training")
axs[0].plot(history_2.history["val_loss"], label="validation")
axs[0].set_xlabel("Epoch")
axs[0].set_ylabel("Loss")
axs[0].legend()

axs[1].plot(history_2.history["loss"], label="training")
axs[1].plot(history_2.history["val_loss"], label="validation")
axs[1].set_xlabel("Epoch")
axs[1].set_ylabel("Mean Squared Error")
axs[1].legend()

fig.suptitle("LOO dataset 2")

fig.show()
```

```{python, train-3}
fig, axs = plt.subplots(1, 2, figsize=(10, 4))

axs[0].plot(history_3.history["loss"], label="training")
axs[0].plot(history_3.history["val_loss"], label="validation")
axs[0].set_xlabel("Epoch")
axs[0].set_ylabel("Loss")
axs[0].legend()

axs[1].plot(history_3.history["loss"], label="training")
axs[1].plot(history_3.history["val_loss"], label="validation")
axs[1].set_xlabel("Epoch")
axs[1].set_ylabel("Mean Squared Error")
axs[1].legend()

fig.suptitle("LOO dataset 3")

fig.show()
```

```{python, train-4}
fig, axs = plt.subplots(1, 2, figsize=(10, 4))

axs[0].plot(history_4.history["loss"], label="training")
axs[0].plot(history_4.history["val_loss"], label="validation")
axs[0].set_xlabel("Epoch")
axs[0].set_ylabel("Loss")
axs[0].legend()

axs[1].plot(history_4.history["loss"], label="training")
axs[1].plot(history_4.history["val_loss"], label="validation")
axs[1].set_xlabel("Epoch")
axs[1].set_ylabel("Mean Squared Error")
axs[1].legend()

fig.suptitle("LOO dataset 4")

fig.show()
```

```{python, train-5}
fig, axs = plt.subplots(1, 2, figsize=(10, 4))

axs[0].plot(history_5.history["loss"], label="training")
axs[0].plot(history_5.history["val_loss"], label="validation")
axs[0].set_xlabel("Epoch")
axs[0].set_ylabel("Loss")
axs[0].legend()

axs[1].plot(history_5.history["loss"], label="training")
axs[1].plot(history_5.history["val_loss"], label="validation")
axs[1].set_xlabel("Epoch")
axs[1].set_ylabel("Mean Squared Error")
axs[1].legend()

fig.suptitle("LOO dataset 5")

fig.show()
```

```{python, train-6}
fig, axs = plt.subplots(1, 2, figsize=(10, 4))

axs[0].plot(history_6.history["loss"], label="training")
axs[0].plot(history_6.history["val_loss"], label="validation")
axs[0].set_xlabel("Epoch")
axs[0].set_ylabel("Loss")
axs[0].legend()

axs[1].plot(history_6.history["loss"], label="training")
axs[1].plot(history_6.history["val_loss"], label="validation")
axs[1].set_xlabel("Epoch")
axs[1].set_ylabel("Mean Squared Error")
axs[1].legend()

fig.suptitle("LOO dataset 6")

fig.show()
```