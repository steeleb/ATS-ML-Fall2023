---
title: "Neural Networks Workspace"
format: html
editor: visual
jupyter: python3
---

```{r env-set-up}
library(tidyverse)
library(reticulate)
library(kableExtra)

# file paths
is_data = '~/OneDrive - Colostate/NASA-Northern/data/waterQuality/harmonized/'
rs_data = '~/OneDrive - Colostate/NASA-Northern/data/remoteSensing/estimates/'

# activate conda env
use_condaenv('~/miniconda3/envs/env_ATSML/')

```

```{python, import-modules}
#| echo: false
import sys
import numpy as np
import seaborn as sb

import pandas as pd
import datetime
import tensorflow as tf
import sklearn

# import pydot
import matplotlib.pyplot as plt
import matplotlib.patches as patches

print(f"python version = {sys.version}")
print(f"numpy version = {np.__version__}")
print(f"tensorflow version = {tf.__version__}")

# set random state
rs = 37
```

## Read in temp data and make train-val-test 

```{r raw_data}
# read in temp data
NW_temp <- read_csv(file.path(is_data, 'manual_temperature_data_NW_harmonized_v2023-08-30.csv'))
surf_temp <- NW_temp %>% 
  group_by(date, feature) %>% 
  arrange(depth) %>% 
  slice(1) %>% 
  filter(station %in% c('CL-DAM1', 'GR-DAM', 'GL-MID', 'HT-DIX', 
                        'SM-DAM', 'WC-DAM', 'WG-DAM'))
  
NW_estimates <- read_csv(file.path(rs_data, 'SurfTemp_linearCorrection_v2023-09-28.csv')) %>% 
  rename(feature = GNIS_Name) %>% 
  mutate(value = adj_medTemp,
         feature = case_when(feature == 'Lake Granby' ~ 'Granby Reservoir',
                             feature == 'Carter Lake Reservoir' ~ 'Carter Lake',
                             feature == 'Shadow Mountain Lake' ~ 'Shadow Mountain Reservoir',
                             TRUE ~ feature),
         station = 'sat') %>% 
  filter(location_type == 'poi_center')

all_NW_temp <- full_join(surf_temp, NW_estimates)

# weather data
weather <- read_csv('~/OneDrive - Colostate/NASA-Northern/data/climate/aggregated/NW_NLDAS_climate_1-3-5d_previous_1984-01-01_2023-05-17_v2023-05-25.csv') %>% 
  rename(feature = lake) %>% 
  pivot_longer(cols = c('tot_precip_mm', 'max_temp_degC', 'mean_temp_degC', 
                        'min_temp_degC', 'tot_sol_rad_Wpm2', 'min_wind_mps',
                        'mean_wind_mps', 'max_wind_mps'),
               names_to = 'variable') %>% 
  pivot_wider(names_from = c('variable', 'n_prev_days'),
              names_sep = '_',
              values_from = 'value') %>% 
  mutate(feature = if_else(feature == 'Lake Granby',
                           'Granby Reservoir',
                           feature))
# static
static <- read_csv('~/OneDrive - Colostate/NASA-Northern/data/static_vars/static_vars_7_lakes.csv')

# join together for full dataset
full_dataset = left_join(static, all_NW_temp) %>% 
  left_join(., weather) %>% 
  filter(between(month, 4, 10)) %>% 
  select(-c(med_SurfaceTemp, adj_medTemp, depth, time, parameter, month,
            station, Latitude, Longitude, lakeID, HarmonizedName, 
            location_type, mission)) %>% 
  arrange(date) %>% 
  filter(complete.cases(.)) 

# drop windy gap for incomplete
full_dataset <- full_dataset %>% 
  filter(feature != 'Windy Gap Reservoir') %>% 
  mutate(day_of_year = yday(date))

```

## Make train-val-test sets

Test data will be from 2021 forward (2y of data)

```{r tst}
test <- full_dataset %>% 
  filter(date >= ymd('2021-01-01'))

```

### make a few iterations of train-val

One will be leave-one-out by lake, the other splitting by timeseries in \~10y increments

```{r train_val}
#full training set
training <- anti_join(full_dataset, test)

# using a leave-one-out method for train/validate
train1 <- training %>% 
  filter(feature != 'Grand Lake')
val1 = anti_join(training, train1)
train2 <- training %>% 
  filter(feature != 'Horsetooth Reservoir')
val2 = anti_join(training, train2)
train3 <- training %>% 
  filter(feature != 'Shadow Mountain Reservoir')
val3 = anti_join(training, train3)
train4 <- training %>% 
  filter(feature != 'Granby Reservoir')
val4 = anti_join(training, train4)
train5 <- training %>% 
  filter(feature != 'Carter Lake')
val5 = anti_join(training, train5)
train6 <- training %>% 
  filter(feature != 'Willow Creek Reservoir')
val6 = anti_join(training, train6)

# using time blocks
train1_ts <- training %>% 
  filter(!between(date, ymd('1984-01-01'), ymd('1995-01-01')))
val1_ts = anti_join(training, train1_ts)
train2_ts <- training %>% 
  filter(!between(date, ymd('1995-01-01'), ymd('2005-01-01')))
val2_ts = anti_join(training, train2_ts)
train3_ts <- training %>% 
  filter(!between(date, ymd('2005-01-01'), ymd('2015-01-01')))
val3_ts = anti_join(training, train3_ts)
train4_ts <- training %>% 
  filter(!between(date, ymd('2015-01-01'), ymd('2021-01-01')))
val4_ts = anti_join(training, train4_ts)

```

### Bring over LOO to py

```{python, label-feature-setup}
#| echo: false

# grab the values we want to predict
labels_1 = np.array(r.train1['value'])
labels_2 = np.array(r.train2['value'])
labels_3 = np.array(r.train3['value'])
labels_4 = np.array(r.train4['value'])
labels_5 = np.array(r.train5['value'])
labels_6 = np.array(r.train6['value'])

# grab the values we want to predict
val_labels_1 = np.array(r.val1['value'])
val_labels_2 = np.array(r.val2['value'])
val_labels_3 = np.array(r.val3['value'])
val_labels_4 = np.array(r.val4['value'])
val_labels_5 = np.array(r.val5['value'])
val_labels_6 = np.array(r.val6['value'])

# and remove the labels from the dataset containing the feature set
features1 = (r.train1
  .drop(['value', 'feature', 'date'], axis = 1))
features2 = (r.train2
  .drop(['value', 'feature', 'date'], axis = 1))
features3 = (r.train3
  .drop(['value', 'feature', 'date'], axis = 1))
features4 = (r.train4
  .drop(['value', 'feature', 'date'], axis = 1))
features5 = (r.train5
  .drop(['value', 'feature', 'date'], axis = 1))
features6 = (r.train6
  .drop(['value', 'feature', 'date'], axis = 1))

# and remove the labels from the dataset containing the feature set
val_features1 = (r.val1
  .drop(['value', 'feature', 'date'], axis = 1))
val_features2 = (r.val2
  .drop(['value', 'feature', 'date'], axis = 1))
val_features3 = (r.val3
  .drop(['value', 'feature', 'date'], axis = 1))
val_features4 = (r.val4
  .drop(['value', 'feature', 'date'], axis = 1))
val_features5 = (r.val5
  .drop(['value', 'feature', 'date'], axis = 1))
val_features6 = (r.val6
  .drop(['value', 'feature', 'date'], axis = 1))

# Saving feature names for later use
feature_list = list(features1.columns)

# Convert to numpy array
features1 = np.array(features1)
features2 = np.array(features2)
features3 = np.array(features3)
features4 = np.array(features4)
features5 = np.array(features5)
features6 = np.array(features6)

# Convert to numpy array
val_features1 = np.array(val_features1)
val_features2 = np.array(val_features2)
val_features3 = np.array(val_features3)
val_features4 = np.array(val_features4)
val_features5 = np.array(val_features5)
val_features6 = np.array(val_features6)


```

### And for ts chunks

```{python timeseries-test-val}
#| echo: false

# grab the values we want to predict
labels_1_ts = np.array(r.train1_ts['value'])
labels_2_ts = np.array(r.train2_ts['value'])
labels_3_ts = np.array(r.train3_ts['value'])
labels_4_ts = np.array(r.train4_ts['value'])

# grab the values we want to predict
val_labels_1_ts = np.array(r.val1_ts['value'])
val_labels_2_ts = np.array(r.val2_ts['value'])
val_labels_3_ts = np.array(r.val3_ts['value'])
val_labels_4_ts = np.array(r.val4_ts['value'])

# and remove the labels from the dataset containing the feature set
features1_ts = (r.train1_ts
  .drop(['value', 'feature', 'date'], axis = 1))
features2_ts = (r.train2_ts
  .drop(['value', 'feature', 'date'], axis = 1))
features3_ts = (r.train3_ts
  .drop(['value', 'feature', 'date'], axis = 1))
features4_ts = (r.train4_ts
  .drop(['value', 'feature', 'date'], axis = 1))

# and remove the labels from the dataset containing the feature set
val_features1_ts = (r.val1_ts
  .drop(['value', 'feature', 'date'], axis = 1))
val_features2_ts = (r.val2_ts
  .drop(['value', 'feature', 'date'], axis = 1))
val_features3_ts = (r.val3_ts
  .drop(['value', 'feature', 'date'], axis = 1))
val_features4_ts = (r.val4_ts
  .drop(['value', 'feature', 'date'], axis = 1))

# Convert to numpy array
features1_ts = np.array(features1_ts)
features2_ts = np.array(features2_ts)
features3_ts = np.array(features3_ts)
features4_ts = np.array(features4_ts)

# Convert to numpy array
val_features1_ts = np.array(val_features1_ts)
val_features2_ts = np.array(val_features2_ts)
val_features3_ts = np.array(val_features3_ts)
val_features4_ts = np.array(val_features4_ts)

```

### Make a baseline dataset

```{r baseline}
baseline_by_date <- full_dataset %>% 
  group_by(day_of_year) %>% 
  summarize(mean_temp_by_date_deg_C = mean(value),
            n = n()) %>% 
  left_join(full_dataset, .) %>% 
  #remove days where there are less than or equal to 3 observations contributing to the mean
  filter(n > 3)

```

### Calculate baseline error

```{python, baseline}
#| echo: false
baseline_day = r.baseline_by_date
mae_baseline_day_errors = np.mean(abs(baseline_day['value'] - baseline_day['mean_temp_by_date_deg_C']))
baseline_mae_err_text = round(mae_baseline_day_errors, 2)

mse_baseline_day_errors = np.sqrt(np.mean(abs(baseline_day['value'] - baseline_day['mean_temp_by_date_deg_C'])**2))
baseline_mse_err_text = round(mse_baseline_day_errors, 2)
```

## Quick look at output

```{python}
sb.displot(labels_1, kind="hist")
plt.show()

sb.displot(labels_2, kind="hist")
plt.show()

sb.displot(labels_3, kind="hist")
plt.show()

sb.displot(labels_4, kind="hist")
plt.show()

sb.displot(labels_5, kind="hist")
plt.show()

sb.displot(labels_6, kind="hist")
plt.show()

sb.displot(labels_1_ts, kind="hist")
plt.show()

sb.displot(labels_2_ts, kind="hist")
plt.show()

sb.displot(labels_3_ts, kind="hist")
plt.show()

sb.displot(labels_4_ts, kind="hist")
plt.show()
```
