{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script creates a very overfit model for predicting surface temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#high level modules\n",
    "import os\n",
    "import sys\n",
    "import imp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# ml/ai modules\n",
    "import tensorflow as tf\n",
    "# Let's import some different things we will use to build the neural network\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout, Softmax\n",
    "\n",
    "# import custom modules\n",
    "this_dir = \"/Users/steeleb/Documents/GitHub/ATS-ML-Fall2023/\"\n",
    "imp.load_source(\"settings\",os.path.join(this_dir,\"NeuralNetworks/settings.py\"))\n",
    "from settings import settings\n",
    "imp.load_source(\"tvt\", os.path.join(this_dir, \"NeuralNetworks/preprocessing.py\"))\n",
    "from tvt import train1, val1, train2, val2, train3, val3, train4, val4, train5, val5, train6, val6\n",
    "from tvt import train1_ts, val1_ts, train2_ts, val2_ts, train3_ts, val3_ts, train4_ts, val4_ts\n",
    "imp.load_source(\"architecture\", os.path.join(this_dir, \"NeuralNetworks/architecture.py\"))\n",
    "from architecture import build_model, compile_model\n",
    "imp.load_source(\"universals\", os.path.join(this_dir, \"NeuralNetworks/universal_functions.py\"))\n",
    "from universals import save_to_pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format training and validation arrays for use in model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# grab the values we want to predict\n",
    "labels_1 = np.array(train1['value'])\n",
    "labels_2 = np.array(train2['value'])\n",
    "labels_3 = np.array(train3['value'])\n",
    "labels_4 = np.array(train4['value'])\n",
    "labels_5 = np.array(train5['value'])\n",
    "labels_6 = np.array(train6['value'])\n",
    "\n",
    "# grab the values we want to predict\n",
    "val_labels_1 = np.array(val1['value'])\n",
    "val_labels_2 = np.array(val2['value'])\n",
    "val_labels_3 = np.array(val3['value'])\n",
    "val_labels_4 = np.array(val4['value'])\n",
    "val_labels_5 = np.array(val5['value'])\n",
    "val_labels_6 = np.array(val6['value'])\n",
    "\n",
    "# and remove the labels from the dataset containing the feature set\n",
    "features1 = (train1\n",
    "  .drop(['value', 'feature', 'date'], axis = 1))\n",
    "features2 = (train2\n",
    "  .drop(['value', 'feature', 'date'], axis = 1))\n",
    "features3 = (train3\n",
    "  .drop(['value', 'feature', 'date'], axis = 1))\n",
    "features4 = (train4\n",
    "  .drop(['value', 'feature', 'date'], axis = 1))\n",
    "features5 = (train5\n",
    "  .drop(['value', 'feature', 'date'], axis = 1))\n",
    "features6 = (train6\n",
    "  .drop(['value', 'feature', 'date'], axis = 1))\n",
    "\n",
    "# and remove the labels from the dataset containing the feature set\n",
    "val_features1 = (val1\n",
    "  .drop(['value', 'feature', 'date'], axis = 1))\n",
    "val_features2 = (val2\n",
    "  .drop(['value', 'feature', 'date'], axis = 1))\n",
    "val_features3 = (val3\n",
    "  .drop(['value', 'feature', 'date'], axis = 1))\n",
    "val_features4 = (val4\n",
    "  .drop(['value', 'feature', 'date'], axis = 1))\n",
    "val_features5 = (val5\n",
    "  .drop(['value', 'feature', 'date'], axis = 1))\n",
    "val_features6 = (val6\n",
    "  .drop(['value', 'feature', 'date'], axis = 1))\n",
    "\n",
    "# Saving feature names for later use\n",
    "feature_list = list(features1.columns)\n",
    "\n",
    "# Convert to numpy array\n",
    "features1 = np.array(features1)\n",
    "features2 = np.array(features2)\n",
    "features3 = np.array(features3)\n",
    "features4 = np.array(features4)\n",
    "features5 = np.array(features5)\n",
    "features6 = np.array(features6)\n",
    "\n",
    "# Convert to numpy array\n",
    "val_features1 = np.array(val_features1)\n",
    "val_features2 = np.array(val_features2)\n",
    "val_features3 = np.array(val_features3)\n",
    "val_features4 = np.array(val_features4)\n",
    "val_features5 = np.array(val_features5)\n",
    "val_features6 = np.array(val_features6)\n",
    "\n",
    "\n",
    "# grab the values we want to predict\n",
    "labels_1_ts = np.array(train1_ts['value'])\n",
    "labels_2_ts = np.array(train2_ts['value'])\n",
    "labels_3_ts = np.array(train3_ts['value'])\n",
    "labels_4_ts = np.array(train4_ts['value'])\n",
    "\n",
    "# grab the values we want to predict\n",
    "val_labels_1_ts = np.array(val1_ts['value'])\n",
    "val_labels_2_ts = np.array(val2_ts['value'])\n",
    "val_labels_3_ts = np.array(val3_ts['value'])\n",
    "val_labels_4_ts = np.array(val4_ts['value'])\n",
    "\n",
    "# and remove the labels from the dataset containing the feature set\n",
    "features1_ts = (train1_ts\n",
    "  .drop(['value', 'feature', 'date'], axis = 1))\n",
    "features2_ts = (train2_ts\n",
    "  .drop(['value', 'feature', 'date'], axis = 1))\n",
    "features3_ts = (train3_ts\n",
    "  .drop(['value', 'feature', 'date'], axis = 1))\n",
    "features4_ts = (train4_ts\n",
    "  .drop(['value', 'feature', 'date'], axis = 1))\n",
    "\n",
    "# and remove the labels from the dataset containing the feature set\n",
    "val_features1_ts = (val1_ts\n",
    "  .drop(['value', 'feature', 'date'], axis = 1))\n",
    "val_features2_ts = (val2_ts\n",
    "  .drop(['value', 'feature', 'date'], axis = 1))\n",
    "val_features3_ts = (val3_ts\n",
    "  .drop(['value', 'feature', 'date'], axis = 1))\n",
    "val_features4_ts = (val4_ts\n",
    "  .drop(['value', 'feature', 'date'], axis = 1))\n",
    "\n",
    "# Convert to numpy array\n",
    "features1_ts = np.array(features1_ts)\n",
    "features2_ts = np.array(features2_ts)\n",
    "features3_ts = np.array(features3_ts)\n",
    "features4_ts = np.array(features4_ts)\n",
    "\n",
    "# Convert to numpy array\n",
    "val_features1_ts = np.array(val_features1_ts)\n",
    "val_features2_ts = np.array(val_features2_ts)\n",
    "val_features3_ts = np.array(val_features3_ts)\n",
    "val_features4_ts = np.array(val_features4_ts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile and train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 32)]              0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 32)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 30)                990       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 30)                930       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 30)                930       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 30)                930       \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 30)                930       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 31        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,741\n",
      "Trainable params: 4,741\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-09 15:07:05.070804: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - ETA: 0s - loss: 0.9910"
     ]
    }
   ],
   "source": [
    "imp.reload(sys.modules['settings'])\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "tf.keras.utils.set_random_seed(settings[\"super_overfit\"][\"random_seed\"])\n",
    "\n",
    "# define the early stopping callback\n",
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "  monitor=\"val_loss\", \n",
    "  patience=settings[\"super_overfit\"][\"patience\"], \n",
    "  restore_best_weights=True, \n",
    "  mode=\"auto\"\n",
    ")\n",
    "\n",
    "## LOO 1\n",
    "model_1 = build_model(\n",
    "  features1, \n",
    "  labels_1, \n",
    "  settings[\"super_overfit\"])\n",
    "\n",
    "model_1 = compile_model(\n",
    "  model_1, \n",
    "  settings['super_overfit'])\n",
    "\n",
    "# train the model via model.fit\n",
    "history_1 = model_1.fit(\n",
    "  features1, \n",
    "  labels_1, \n",
    "  epochs=settings['super_overfit'][\"max_epochs\"],\n",
    "  batch_size=settings['super_overfit'][\"batch_size\"],\n",
    "  shuffle=True,\n",
    "  validation_data=[val_features1, val_labels_1],\n",
    "  callbacks=[early_stopping_callback],\n",
    "  verbose=1,\n",
    ")\n",
    "\n",
    "## LOO 2\n",
    "model_2 = build_model(\n",
    "  features2,\n",
    "  labels_2, \n",
    "  settings[\"super_overfit\"])\n",
    "model_2 = compile_model(model_2, settings[\"super_overfit\"])\n",
    "\n",
    "# train the model via model.fit\n",
    "history_2 = model_2.fit(\n",
    "  features2,\n",
    "  labels_2,\n",
    "  epochs=settings[\"super_overfit\"][\"max_epochs\"],\n",
    "  batch_size=settings[\"super_overfit\"][\"batch_size\"],\n",
    "  shuffle=True,\n",
    "  validation_data=[val_features2, val_labels_2],\n",
    "  callbacks=[early_stopping_callback],\n",
    "  verbose=1,\n",
    ")\n",
    "\n",
    "## LOO 3\n",
    "\n",
    "model_3 = build_model(\n",
    "  features3,\n",
    "  labels_3,\n",
    "  settings[\"super_overfit\"])\n",
    "model_3 = compile_model(model_3, settings[\"super_overfit\"])\n",
    "\n",
    "# train the model via model.fit\n",
    "history_3 = model_3.fit(\n",
    "  features3,\n",
    "  labels_3,\n",
    "  epochs=settings[\"super_overfit\"][\"max_epochs\"],\n",
    "  batch_size=settings[\"super_overfit\"][\"batch_size\"],\n",
    "  shuffle=True,\n",
    "  validation_data=[val_features3, val_labels_3],\n",
    "  callbacks=[early_stopping_callback],\n",
    "  verbose=1,\n",
    ")\n",
    "\n",
    "## LOO 4\n",
    "\n",
    "model_4 = build_model(\n",
    "  features4,\n",
    "  labels_4,\n",
    "  settings[\"super_overfit\"])\n",
    "model_4 = compile_model(model_4, settings[\"super_overfit\"])\n",
    "\n",
    "# train the model via model.fit\n",
    "history_4 = model_4.fit(\n",
    "  features4,\n",
    "  labels_4,\n",
    "  epochs=settings[\"super_overfit\"][\"max_epochs\"],\n",
    "  batch_size=settings[\"super_overfit\"][\"batch_size\"],\n",
    "  shuffle=True,\n",
    "  validation_data=[val_features4, val_labels_4],\n",
    "  callbacks=[early_stopping_callback],\n",
    "  verbose=1,\n",
    ")\n",
    "\n",
    "## LOO 5\n",
    "\n",
    "model_5 = build_model(\n",
    "  features5,\n",
    "  labels_5,\n",
    "  settings[\"super_overfit\"])\n",
    "model_5 = compile_model(model_5, settings[\"super_overfit\"])\n",
    "\n",
    "# train the model via model.fit\n",
    "history_5 = model_5.fit(\n",
    "  features5,\n",
    "  labels_5,\n",
    "  epochs=settings[\"super_overfit\"][\"max_epochs\"],\n",
    "  batch_size=settings[\"super_overfit\"][\"batch_size\"],\n",
    "  shuffle=True,\n",
    "  validation_data=[val_features5, val_labels_5],\n",
    "  callbacks=[early_stopping_callback],\n",
    "  verbose=1,\n",
    ")\n",
    "\n",
    "## LOO 6\n",
    "\n",
    "model_6 = build_model(\n",
    "  features6,\n",
    "  labels_6,\n",
    "  settings[\"super_overfit\"])\n",
    "model_6 = compile_model(model_6, settings[\"super_overfit\"])\n",
    "\n",
    "# train the model via model.fit\n",
    "history_6 = model_6.fit(\n",
    "  features6,\n",
    "  labels_6,\n",
    "  epochs=settings[\"super_overfit\"][\"max_epochs\"],\n",
    "  batch_size=settings[\"super_overfit\"][\"batch_size\"],\n",
    "  shuffle=True,\n",
    "  validation_data=[val_features6, val_labels_6],\n",
    "  callbacks=[early_stopping_callback],\n",
    "  verbose=1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And save the models and training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_dir = \"/Users/steeleb/OneDrive - Colostate/NASA-Northern/data/NN_train_val_test/models/super_overfit/\"\n",
    "\n",
    "# save models to pickle\n",
    "models = [model_1, model_2, model_3, model_4, model_5, model_6]\n",
    "\n",
    "for model, i in zip(models, range(1,7)):\n",
    "    save_to_pickle(model, f\"{dump_dir}/model_{i}.pkl\")\n",
    "\n",
    "# save history to pickles\n",
    "histories = [history_1, history_2, history_3, history_4, history_5, history_6]\n",
    "\n",
    "for history, i in zip(histories, range(1,7)):\n",
    "    save_to_pickle(history, f\"{dump_dir}/history_{i}.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then do the same for timeseries train/val sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tf.keras.backend.clear_session()\n",
    "tf.keras.utils.set_random_seed(settings[\"super_overfit\"][\"random_seed\"])\n",
    "\n",
    "# define the early stopping callback\n",
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "  monitor=\"val_loss\", \n",
    "  patience=settings[\"super_overfit\"][\"patience\"], \n",
    "  restore_best_weights=True, \n",
    "  mode=\"auto\"\n",
    ")\n",
    "\n",
    "## LOO 1\n",
    "model_1_ts = build_model(\n",
    "  features1_ts, \n",
    "  labels_1_ts, \n",
    "  settings[\"super_overfit\"])\n",
    "\n",
    "model_1_ts = compile_model(\n",
    "  model_1_ts, \n",
    "  settings['super_overfit'])\n",
    "\n",
    "# train the model via model.fit\n",
    "history_1_ts = model_1_ts.fit(\n",
    "  features1_ts, \n",
    "  labels_1_ts, \n",
    "  epochs=settings['super_overfit'][\"max_epochs\"],\n",
    "  batch_size=settings['super_overfit'][\"batch_size\"],\n",
    "  shuffle=True,\n",
    "  validation_data=[val_features1_ts, val_labels_1_ts],\n",
    "  callbacks=[early_stopping_callback],\n",
    "  verbose=1,\n",
    ")\n",
    "\n",
    "## LOO 2\n",
    "model_2_ts = build_model(\n",
    "  features2_ts,\n",
    "  labels_2_ts, \n",
    "  settings[\"super_overfit\"])\n",
    "model_2_ts = compile_model(model_2_ts, settings[\"super_overfit\"])\n",
    "\n",
    "# train the model via model.fit\n",
    "history_2_ts = model_2_ts.fit(\n",
    "  features2_ts,\n",
    "  labels_2_ts,\n",
    "  epochs=settings[\"super_overfit\"][\"max_epochs\"],\n",
    "  batch_size=settings[\"super_overfit\"][\"batch_size\"],\n",
    "  shuffle=True,\n",
    "  validation_data=[val_features2_ts, val_labels_2_ts],\n",
    "  callbacks=[early_stopping_callback],\n",
    "  verbose=1,\n",
    ")\n",
    "\n",
    "## LOO 3\n",
    "\n",
    "model_3_ts = build_model(\n",
    "  features3_ts,\n",
    "  labels_3_ts,\n",
    "  settings[\"super_overfit\"])\n",
    "model_3_ts = compile_model(model_3_ts, settings[\"super_overfit\"])\n",
    "\n",
    "# train the model via model.fit\n",
    "history_3_ts = model_3_ts.fit(\n",
    "  features3_ts,\n",
    "  labels_3_ts,\n",
    "  epochs=settings[\"super_overfit\"][\"max_epochs\"],\n",
    "  batch_size=settings[\"super_overfit\"][\"batch_size\"],\n",
    "  shuffle=True,\n",
    "  validation_data=[val_features3_ts, val_labels_3_ts],\n",
    "  callbacks=[early_stopping_callback],\n",
    "  verbose=1,\n",
    ")\n",
    "\n",
    "## LOO 4\n",
    "\n",
    "model_4_ts = build_model(\n",
    "  features4_ts,\n",
    "  labels_4_ts,\n",
    "  settings[\"super_overfit\"])\n",
    "model_4_ts = compile_model(model_4_ts, settings[\"super_overfit\"])\n",
    "\n",
    "# train the model via model.fit\n",
    "history_4_ts = model_4_ts.fit(\n",
    "  features4_ts,\n",
    "  labels_4_ts,\n",
    "  epochs=settings[\"super_overfit\"][\"max_epochs\"],\n",
    "  batch_size=settings[\"super_overfit\"][\"batch_size\"],\n",
    "  shuffle=True,\n",
    "  validation_data=[val_features4_ts, val_labels_4_ts],\n",
    "  callbacks=[early_stopping_callback],\n",
    "  verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And save to pickles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save models to pickle\n",
    "ts_models = [model_1_ts, model_2_ts, model_3_ts, model_4_ts]\n",
    "\n",
    "for model, i in zip(ts_models, range(1,7)):\n",
    "    save_to_pickle(model, f\"{dump_dir}/ts_model_{i}.pkl\")\n",
    "\n",
    "# save history to pickles\n",
    "ts_histories = [history_1_ts, history_2_ts, history_3_ts, history_4_ts]\n",
    "\n",
    "for history, i in zip(ts_histories, range(1,7)):\n",
    "    save_to_pickle(history, f\"{dump_dir}/ts_history_{i}.pkl\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_ATSML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
