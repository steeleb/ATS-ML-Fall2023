---
title: "Estimation of Daily Water Temperature using Fully-Connected Neural Networks"
author: "B Steele"
date: today
date-format: long
format: pdf
editor: 
  visual:
    theme: sky
editor_options:
  markdown:
    wrap: 80
---

[GH Repo](https://github.com/steeleb/ATS-ML-Fall2023)

```{r env-set-up, echo=FALSE, message=FALSE}
library(tidyverse)
library(reticulate)
library(kableExtra)
library(Metrics)

# activate conda env
use_condaenv('~/miniconda3/envs/env_ATSML/')

# data directory
dir = "/Users/steeleb/OneDrive - Colostate/NASA-Northern/data/NN_train_val_test/"
```

```{python import-modules}
#| echo: false

import os
import sys
import imp
import numpy as np
import seaborn as sb
import pickle
import pandas as pd
import datetime
import tensorflow as tf
import sklearn
import matplotlib.pyplot as plt

# custom modules



```

## Scientific motivation and problem statement:

Water temperature is often an indicator of water quality, as it governs much of
the biological activity in freshwater systems. While temperature is an important
parameter to monitor in freshwater lakes, manual monitoring of waterbodies (by
physically visiting a site) and sensor networks to monitor water temperature,
are costly endeavors.

In this example, I will use a fully-connected neural network to estimate water
surface temperature for reservoirs with long manual monitoring data from
Northern Water, the municipal subdistrict that delivers drinking water to
approximately 1 million people in northern Colorado and irrigation water for
\~600,000 acres of land. The features that I will be using to estimate surface
temperature include summary NLDAS meteorological data (air temperature,
precipitation, solar radiation, and wind) as well as static values for each of
the reservoirs (elevation, surface area, maximum depth, volume, and shoreline
distance). The NLDAS data have been summarized for the previous day's weather, 3
days prior, and 5 days prior - meaning, the model does not use *today's* weather
for prediction. To capture annual warming and seasonal warm-up/cool-down, which
are not always consistent between annual cycles, I've implemented an annual
cumulative sum for both temperature and solar radiation and the day of year
within the feature set.

```{r baseline, echo=FALSE, message=FALSE}
full_dataset <- read_csv(file.path(dir, "training_set_up_to_2021_v2023-11-08.csv"))
baseline_by_date <- full_dataset %>% 
  group_by(day_of_year) %>% 
  summarize(mean_temp_by_date_deg_C = mean(value),
            n = n()) %>% 
  left_join(full_dataset, .) %>% 
  #remove days where there are less than or equal to 3 observations contributing to the mean
  filter(n > 3)

base_mae = mae(baseline_by_date$value, baseline_by_date$mean_temp_by_date_deg_C)
base_mse = mse(baseline_by_date$value, baseline_by_date$mean_temp_by_date_deg_C)
base_rmse = rmse(baseline_by_date$value, baseline_by_date$mean_temp_by_date_deg_C)
base_mape = mape(baseline_by_date$value, baseline_by_date$mean_temp_by_date_deg_C)
```

The comparative baseline for this analysis will be the day-of-year average water
temperature across all lakes and years, where there are at least 3 values
contributing to the mean. The baseline estimates result in a MAE of
`r round(base_mae, 2)` deg C, MSE of `r round(base_mse, 2)` deg C, RMSE of
`r round(base_rmse, 2)`, and MAPE of `r round(base_mape*100, 2)`%.

```{r load-ref-files, echo = F, message = F}
is_dir = '~/OneDrive - Colostate/NASA-Northern/data/waterQuality/harmonized/'
surf_temp <- read_csv(file.path(is_dir, 'manual_temperature_data_NW_harmonized_v2023-08-30.csv')) %>% 
  group_by(date, feature) %>% 
  arrange(depth) %>% 
  slice(1) %>% 
  filter(station %in% c('CL-DAM1', 'GR-DAM', 'GL-MID', 'HT-DIX', 
                        'SM-DAM', 'WC-DAM', 'WG-DAM')) %>%
  filter(date < ymd("2023-01-01"))

rs_dir = '~/OneDrive - Colostate/NASA-Northern/data/remoteSensing/estimates/'
NW_estimates <- read_csv(file.path(rs_dir, 'SurfTemp_linearCorrection_v2023-09-28.csv')) %>% 
  rename(feature = GNIS_Name) %>% 
  mutate(value = adj_medTemp,
         feature = case_when(feature == 'Lake Granby' ~ 'Granby Reservoir',
                             feature == 'Carter Lake Reservoir' ~ 'Carter Lake',
                             feature == 'Shadow Mountain Lake' ~ 'Shadow Mountain Reservoir',
                             TRUE ~ feature),
         station = 'sat') %>% 
  filter(location_type == 'poi_center')
```

In addition to the manual sampling record that is maintained by Northern Water
(n = `r nrow(surf_temp)`), I will be leveraging surface temperature estimates
from the Landsat constellation, Landsat 4-9 (n = `r nrow(NW_estimates)`). These
thermal estimates are well-aligned with the manual monitoring data for the 7
reservoirs and have been bias-corrected for over estimates in the warmest
months. 'Surface temperature' in the manual sampling record for this example is
any measured temperature at \>= 1m depth. I retain only the top-most value for
temperature. Static variables are only available for 6 of 7 reservoirs, so Windy
Gap reservoir has been dropped from this analysis (loss of
`r nrow(surf_temp %>% filter(feature == "Windy Gap Reservoir"))` and
`r nrow(NW_estimates %>% filter(feature == "Windy Gap Reservoir"))` rows in
aforementioned datasets).

## Training/Validation/Testing

### Preprocessing

All precipitation data are right skewed heavily biased to low precip values
including zero, to account for this and make the distribution more normal, I
added 0.0001 to each value and applied a log-10 transformation to this subset.
The wind data were left skewed and to transform the distribution, I used a
square root transformation. All features and inputs were then scaled using the
mean and standard deviation to get the values closer around zero, which are
preferable for neural networks.

### Train-validation-test split

Eventual implementation of this algorithm will include forecasting of
temperature for these lakes as well as lakes that have only Landsat-derived
temperature estimates and that are not included in this dataset. Because I want
this algorithm to perform well on new lakes, I want to take steps to make sure
that the algorithm is not overfit to these specific lakes static
characteristics. While this information may be important for alogorithm
development, the model may have a propensity to "learn" those key attributes and
overfit to the data, not allowing for generalization beyond these lakes.

For training and validation I will use two techniques. First, a leave-one-out
method that will result in six NN models where each iteration will use data from
a single lake for validation and the other five for training. Because the random
forest models did not appear to overfit to the static variables, I'm also trying
a timeseries method that will subset the data into \~10 year increments and
leave one increment out per training and use it for validation per iteration.
Since the intended implementation will be daily forecasts, testing performance
will be assessed through hindcasting. The hindcast dataset is a holdout dataset
beginning in 2021 across all lakes.

## Results

```{python load-models}

```

### Leave one out validations

These are super hit-or-miss. MSE ranges from 3-10. Pretty terrible.

```{r loo-val-figs, echo=F}
# ggplot(val1, aes(x = date, y = value)) +
#   geom_point() +
#   geom_point(aes(y = pred), color = 'orange') +
#   facet_grid(feature ~ .) +
#   theme_bw()
# 
# ggplot(val2, aes(x = date, y = value)) +
#   geom_point() +
#   geom_point(aes(y = pred), color = 'orange') +
#   facet_grid(feature ~ .) +
#   theme_bw()
# 
# ggplot(val3, aes(x = date, y = value)) +
#   geom_point() +
#   geom_point(aes(y = pred), color = 'orange') +
#   facet_grid(feature ~ .) +
#   theme_bw()
# 
# ggplot(val4, aes(x = date, y = value)) +
#   geom_point() +
#   geom_point(aes(y = pred), color = 'orange') +
#   facet_grid(feature ~ .) +
#   theme_bw()
# 
# ggplot(val5, aes(x = date, y = value)) +
#   geom_point() +
#   geom_point(aes(y = pred), color = 'orange') +
#   facet_grid(feature ~ .) +
#   theme_bw()
# 
# ggplot(val6, aes(x = date, y = value)) +
#   geom_point() +
#   geom_point(aes(y = pred), color = 'orange') +
#   facet_grid(feature ~ .) +
#   theme_bw()

```

### Timeseries split validations

These are good until the most recent data, which look horrid. MSE is smaller,
but still higher than baseline (3-5).

```{r ts-val-figs, echo = F}
# ggplot(val1_ts, aes(x = date, y = value)) +
#   geom_point() +
#   geom_point(aes(y = pred), color = 'orange') +
#   facet_grid(feature ~ .) +
#   theme_bw()
# 
# ggplot(val2_ts, aes(x = date, y = value)) +
#   geom_point() +
#   geom_point(aes(y = pred), color = 'orange') +
#   facet_grid(feature ~ .) +
#   theme_bw()
# 
# ggplot(val3_ts, aes(x = date, y = value)) +
#   geom_point() +
#   geom_point(aes(y = pred), color = 'orange') +
#   facet_grid(feature ~ .) +
#   theme_bw()
# 
# ggplot(val4_ts, aes(x = date, y = value)) +
#   geom_point() +
#   geom_point(aes(y = pred), color = 'orange') +
#   facet_grid(feature ~ .) +
#   theme_bw()
```

### Hyper-parameter tuning

Current settings:

```{python}
#settings["basic"]
```

<!-- ### Hindcasting application -->

<!-- ### Discussion -->

<!-- ## Supporting Figures -->
