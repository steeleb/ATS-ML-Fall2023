---
title: "Estimation of Daily Water Temperature using Neural Networks"
author: "B Steele"
date: today
date-format: long
format: pdf
editor: 
  visual:
    theme: sky
---

[GH Repo](https://github.com/steeleb/ATS-ML-Fall2023)

## Questions:

-   I'd like the train/val figures to be plotted as MSE in degrees, not transformed values. If I use a Normalization layer within the DNN instead of the sklearn pipeline, will these show up as 'real values'? Did I just end up at a dead end for inverse processing of those values?
-   I don't think that the features I am using are sufficient data for this method. I think this is likely an 'I need better features' (and likely I need more data) problem
-   Can I use SHAP on continuous data? If so, would I apply it to the whole model (if it was well-performing?) instead of by 'correct' assignments of category like in [the example code here](https://github.com/eabarnes1010/course_ml_ats/blob/main/code/ann_ozone_joshuatree_filled.ipynb)?

```{r env-set-up, echo=FALSE, message=FALSE}
library(tidyverse)
library(reticulate)
library(kableExtra)

# file paths
is_data = '~/OneDrive - Colostate/NASA-Northern/data/waterQuality/harmonized/'
rs_data = '~/OneDrive - Colostate/NASA-Northern/data/remoteSensing/estimates/'

# activate conda env
use_condaenv('~/miniconda3/envs/env_ATSML/')
```

```{python import-modules}
#| echo: false
import sys
import numpy as np
import seaborn as sb

import pandas as pd
import datetime
import tensorflow as tf
# Let's import some different things we will use to build the neural network
from tensorflow.keras import Model
from tensorflow.keras.layers import Dense, Input, Dropout, Softmax

import sklearn
from sklearn.preprocessing import FunctionTransformer
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.compose import ColumnTransformer

# import pydot
import matplotlib.pyplot as plt
import matplotlib.patches as patches
```

```{r raw_data, echo=FALSE, message=FALSE}
# read in temp data
NW_temp <- read_csv(file.path(is_data, 'manual_temperature_data_NW_harmonized_v2023-08-30.csv'))
surf_temp <- NW_temp %>% 
  group_by(date, feature) %>% 
  arrange(depth) %>% 
  slice(1) %>% 
  filter(station %in% c('CL-DAM1', 'GR-DAM', 'GL-MID', 'HT-DIX', 
                        'SM-DAM', 'WC-DAM', 'WG-DAM'))
  
NW_estimates <- read_csv(file.path(rs_data, 'SurfTemp_linearCorrection_v2023-09-28.csv')) %>% 
  rename(feature = GNIS_Name) %>% 
  mutate(value = adj_medTemp,
         feature = case_when(feature == 'Lake Granby' ~ 'Granby Reservoir',
                             feature == 'Carter Lake Reservoir' ~ 'Carter Lake',
                             feature == 'Shadow Mountain Lake' ~ 'Shadow Mountain Reservoir',
                             TRUE ~ feature),
         station = 'sat') %>% 
  filter(location_type == 'poi_center')

all_NW_temp <- full_join(surf_temp, NW_estimates)

# weather data
weather <- read_csv('~/OneDrive - Colostate/NASA-Northern/data/climate/aggregated/NW_NLDAS_climate_1-3-5d_previous_1984-01-01_2023-05-17_v2023-05-25.csv') %>% 
  rename(feature = lake) %>% 
  pivot_longer(cols = c('tot_precip_mm', 'max_temp_degC', 'mean_temp_degC', 
                        'min_temp_degC', 'tot_sol_rad_Wpm2', 'min_wind_mps',
                        'mean_wind_mps', 'max_wind_mps'),
               names_to = 'variable') %>% 
  pivot_wider(names_from = c('variable', 'n_prev_days'),
              names_sep = '_',
              values_from = 'value') %>% 
  mutate(feature = if_else(feature == 'Lake Granby',
                           'Granby Reservoir',
                           feature))
# static
static <- read_csv('~/OneDrive - Colostate/NASA-Northern/data/static_vars/static_vars_7_lakes.csv')

# join together for full dataset
full_dataset = left_join(static, all_NW_temp) %>% 
  left_join(., weather) %>% 
  filter(between(month, 4, 10)) %>% 
  select(-c(med_SurfaceTemp, adj_medTemp, depth, time, parameter, month,
            station, Latitude, Longitude, lakeID, HarmonizedName, 
            location_type, mission)) %>% 
  arrange(date) %>% 
  filter(complete.cases(.)) 

# drop windy gap for incomplete
full_dataset <- full_dataset %>% 
  filter(feature != 'Windy Gap Reservoir') %>% 
  mutate(day_of_year = yday(date))

```

```{r test-train-r, echo=FALSE, message=FALSE}
test <- full_dataset %>% 
  filter(date >= ymd('2021-01-01')) 

#full training set
training <- anti_join(full_dataset, test) %>% 
  mutate(across(c("tot_precip_mm_1", "tot_precip_mm_3", "tot_precip_mm_5"),
                ~ . + 0.0001))

test <- test %>% 
  mutate(across(c("tot_precip_mm_1", "tot_precip_mm_3", "tot_precip_mm_5"),
                ~ . + 0.0001))
```

```{python test-py}
#| echo: false

test = r.test
# grab the values we want to predict
test_labels = np.array(test['value'])
test_features = (test
  .drop(['value', 'feature', 'date'], axis = 1))
```

```{r make-train-val-sets, echo=FALSE, message=FALSE}
# using a leave-one-out method for train/validate
train1 <- training %>% 
  filter(feature != 'Grand Lake')
val1 = anti_join(training, train1)
train2 <- training %>% 
  filter(feature != 'Horsetooth Reservoir')
val2 = anti_join(training, train2)
train3 <- training %>% 
  filter(feature != 'Shadow Mountain Reservoir')
val3 = anti_join(training, train3)
train4 <- training %>% 
  filter(feature != 'Granby Reservoir')
val4 = anti_join(training, train4)
train5 <- training %>% 
  filter(feature != 'Carter Lake')
val5 = anti_join(training, train5)
train6 <- training %>% 
  filter(feature != 'Willow Creek Reservoir')
val6 = anti_join(training, train6)

# using time blocks
train1_ts <- training %>% 
  filter(!between(date, ymd('1984-01-01'), ymd('1995-01-01')))
val1_ts = anti_join(training, train1_ts)
train2_ts <- training %>% 
  filter(!between(date, ymd('1995-01-01'), ymd('2005-01-01')))
val2_ts = anti_join(training, train2_ts)
train3_ts <- training %>% 
  filter(!between(date, ymd('2005-01-01'), ymd('2015-01-01')))
val3_ts = anti_join(training, train3_ts)
train4_ts <- training %>% 
  filter(!between(date, ymd('2015-01-01'), ymd('2021-01-01')))
val4_ts = anti_join(training, train4_ts)
```

## Scientific motivation and problem statement:

Water temperature is often an indicator of water quality, as it governs much of the biological activity in freshwater. While temperature is an important parameter to monitor in freshwater lakes, manual monitoring of waterbodies (by physically visiting a site) and sensor networks to monitor water temperature, are costly endeavors.

In this example, I will use a simple fully-connected neural network to estimate water surface temperature for reservoirs with long manual monitoring data from Northern Water, the municipal subdistrict that delivers drinking water to approximately 1 million people in northern Colorado and irrigation water for \~600,000 acres of land. The features that I will be using to estimate surface temperature include summary NLDAS meteorological data (air temperature, precipitation, solar radiation, and wind) as well as static values for each of the reservoirs (elevation, surface area, maximum depth, volume, and shoreline distance). To capture seasonal dynamics, I've also included the numerical day of year to the feature set. The NLDAS data have been summarized for the previous day's weather, 3 days prior, and 5 days prior - meaning, the model does not use *today's* weather for prediction.

```{r baseline, echo=FALSE, message=FALSE}
baseline_by_date <- full_dataset %>% 
  group_by(day_of_year) %>% 
  summarize(mean_temp_by_date_deg_C = mean(value),
            n = n()) %>% 
  left_join(full_dataset, .) %>% 
  #remove days where there are less than or equal to 3 observations contributing to the mean
  filter(n > 3)

```

```{python baseline-error}
#| echo: false
baseline_day = r.baseline_by_date
mae_baseline_day_errors = np.mean(abs(baseline_day['value'] - baseline_day['mean_temp_by_date_deg_C']))
baseline_mae_err_text = round(mae_baseline_day_errors, 2)

mse_baseline_day_errors = np.sqrt(np.mean(abs(baseline_day['value'] - baseline_day['mean_temp_by_date_deg_C'])**2))
baseline_mse_err_text = round(mse_baseline_day_errors, 2)
```

The comparative baseline for this analysis will be the day-of-year average water temperature across all lakes and years, where there are at least 3 values contributing to the mean. The baseline estimates result in a MAE of `r py$baseline_mae_err_text` deg C and MSE of `r py$baseline_mse_err_text` deg C.

In addition to the manual sampling record that is maintained by Northern Water (n = `r nrow(surf_temp)`), I will be leveraging surface temperature estimates from the Landsat constellation, Landsat 4-9 (n = `r nrow(NW_estimates)`). These thermal estimates are well-aligned with the manual monitoring data for the 7 reservoirs and have been bias-corrected for over estimates in the warmest months. 'Surface temperature' in the manual sampling record for this example is any measured temperature at \>= 1m depth. I retain only the top-most value for temperature. Static variables are only available for 6 of 7 reservoirs, so Windy Gap reservoir has been dropped from this analysis.

All precipitation data are right skewed heavily biased to low precip values including zero, to account for this and make the distribution more normal, I added 0.001 to each value and applied a square root transformation to this subset. The wind data were left skewed and to transform the distribution, I used a log function. All features and inputs were then scaled using the `StandardScaler()` function to get the values closer to zero, which are preferable for neural networks. These transformations were completed using the `make_pipeline()` function from the sklearn package.

## Training/Validation/Testing

Eventual implementation of this algorithm will include forecasting of temperature for these lakes as well as lakes that have only Landsat-derived temperature estimates and that are not included in this dataset. Because I want this algorithm to perform well on new lakes, I want to take steps to make sure that the algorithm is not overfit to these specific lakes static characteristics. While this information may be important for alogorithm development, the model may have a propensity to "learn" those key attributes and overfit to the data, not allowing for generalization beyond these lakes.

For training and validation I will use two techniques. First, a leave-one-out method that will result in six NN models where each iteration will use data from a single lake for validation and the other five for training. Because the random forest models did not appear to overfit to the static variables, I'm also trying a timeseries method that will subset the data into \~10 year increments and leave one increment out per training and use it for validation per iteration. Since the intended implementation will be daily forecasts, testing performance will be assessed through hindcasting. The hindcast dataset is a holdout dataset beginning in 2021 across all lakes.

```{python make-pre-processing-pipeline}
#| echo: false

log_transformer = FunctionTransformer(np.log, 
  inverse_func=np.exp, 
  feature_names_out = "one-to-one")
sqrt_transformer = FunctionTransformer(np.sqrt, 
  inverse_func=np.square,
  feature_names_out = "one-to-one")

precip_pipeline = make_pipeline(
  log_transformer,
  StandardScaler()
)

wind_pipeline = make_pipeline(
  sqrt_transformer,
  StandardScaler()
)

precip_vars = ["tot_precip_mm_1", "tot_precip_mm_3", "tot_precip_mm_5"]

wind_vars = ["min_wind_mps_1", "mean_wind_mps_1", "max_wind_mps_1",
  "min_wind_mps_3", "mean_wind_mps_3", "max_wind_mps_3",
  "min_wind_mps_5", "mean_wind_mps_5", "max_wind_mps_5"]

std_vars = ["value", "day_of_year",
  "max_temp_degC_1", "mean_temp_degC_1", "min_temp_degC_1", "tot_sol_rad_Wpm2_1",
  "max_temp_degC_3", "mean_temp_degC_3", "min_temp_degC_3", "tot_sol_rad_Wpm2_3", 
  "max_temp_degC_5", "mean_temp_degC_5", "min_temp_degC_5", "tot_sol_rad_Wpm2_5"]

preprocessing = ColumnTransformer([
  ("precip", precip_pipeline, precip_vars),
  ("wind", wind_pipeline, wind_vars),
  ("std", StandardScaler(), std_vars)
])
```

```{python train-val-to-py}
#| echo: false
#bring data over from R
train1 = r.train1
train2 = r.train2
train3 = r.train3
train4 = r.train4
train5 = r.train5
train6 = r.train6
val1 = r.val1
val2 = r.val2
val3 = r.val3
val4 = r.val4
val5 = r.val5
val6 = r.val6

train1_ts = r.train1_ts
train2_ts = r.train2_ts
train3_ts = r.train3_ts
train4_ts = r.train4_ts

val1_ts = r.val1_ts
val2_ts = r.val2_ts
val3_ts = r.val3_ts
val4_ts = r.val4_ts
```

```{python preproc-loo}
#| echo: false
def preproc(train, val, test):
  pp = preprocessing.fit(train)
  train_pp = pp.fit_transform(train)
  test_pp = pp.fit_transform(test)
  val_pp = pp.fit_transform(val)
  train_df = pd.DataFrame(
    train_pp,
    columns = pp.get_feature_names_out(),
    index = train.index
  )
  val_df = pd.DataFrame(
    val_pp,
    columns = pp.get_feature_names_out(),
    index = val.index
  )
  test_df = pd.DataFrame(
    test_pp,
    columns = pp.get_feature_names_out(),
    index = test.index
  )
  return train_df, val_df, test_df

train1_df, val1_df, test_df_1 = preproc(train1, val1, test)
train2_df, val2_df, test_df_2 = preproc(train2, val2, test)
train3_df, val3_df, test_df_3 = preproc(train3, val3, test)
train4_df, val4_df, test_df_4 = preproc(train4, val4, test)
train5_df, val5_df, test_df_5 = preproc(train5, val5, test)
train6_df, val6_df, test_df_6 = preproc(train6, val6, test)
```

```{python preproc-ts}
#| echo: false
train1_ts_df, val1_ts_df, test_df_ts_1 = preproc(train1_ts, val1_ts, test)
train2_ts_df, val2_ts_df, test_df_ts_2 = preproc(train2_ts, val2_ts, test)
train3_ts_df, val3_ts_df, test_df_ts_3 = preproc(train3_ts, val3_ts, test)
train4_ts_df, val4_ts_df, test_df_ts_4 = preproc(train4_ts, val4_ts, test)
```

```{python build-compile-model}
#| echo: false
def build_model(x_train, y_train, settings):
  # create input layer
  input_layer = tf.keras.layers.Input(shape=x_train.shape[1:])
  
  # # create hidden layers each with specific number of nodes
  # assert len(settings["hiddens"]) == len(
  #   settings["activations"]
  # ), "hiddens and activations settings must be the same length."
  
  # add dropout layer
  layers = tf.keras.layers.Dropout(rate=settings["dropout_rate"])(input_layer)
  
  for hidden, activation in zip(settings["hiddens"], settings["activations"]):
    layers = tf.keras.layers.Dense(
      units=hidden,
      activation=activation,
      use_bias=True,
      kernel_regularizer=tf.keras.regularizers.l1_l2(l1=0, l2=0),
      bias_initializer=tf.keras.initializers.RandomNormal(seed=settings["random_seed"]),
      kernel_initializer=tf.keras.initializers.RandomNormal(seed=settings["random_seed"]),
      )(layers)
  
  # create output layer
  output_layer = tf.keras.layers.Dense(
    units=1,
    activation="linear",
    use_bias=True,
    bias_initializer=tf.keras.initializers.RandomNormal(seed=settings["random_seed"] + 1),
    kernel_initializer=tf.keras.initializers.RandomNormal(seed=settings["random_seed"] + 2),
  )(layers)
  
  # construct the model
  model = tf.keras.Model(inputs=input_layer, outputs=output_layer)
  model.summary()
  
  return model


def compile_model(model, settings):
  model.compile(
      optimizer=tf.keras.optimizers.Adam(
        learning_rate=settings["learning_rate"],
        ),
      loss=tf.keras.losses.MeanSquaredError()
  )
  return model
```

```{python hyperparm-settings-train}
#| echo: false
settings = {
    "hiddens": [3, 3],
    "activations": ["relu", "relu"],
    "learning_rate": 0.001,
    "random_seed": 57,
    "max_epochs": 1000,
    "batch_size": 32,
    "patience": 10,
    "dropout_rate": 0,
}

tf.keras.backend.clear_session()
tf.keras.utils.set_random_seed(settings["random_seed"])

# define the early stopping callback
early_stopping_callback = tf.keras.callbacks.EarlyStopping(
  monitor="val_loss", 
  patience=settings["patience"], 
  restore_best_weights=True, 
  mode="auto"
)

```

```{python build-LOO}
#| echo: false
#| output: false
model_1 = build_model(
  train1_df.drop('std__value', axis = 1), 
  train1_df['std__value'], 
  settings)
model_1 = compile_model(model_1, settings)

# train the model via model.fit
history_1 = model_1.fit(
  train1_df.drop('std__value', axis = 1), 
  train1_df['std__value'], 
  epochs=settings["max_epochs"],
  batch_size=settings["batch_size"],
  shuffle=True,
  validation_data=[val1_df.drop('std__value', axis = 1), val1_df['std__value']],
  callbacks=[early_stopping_callback],
  verbose=1,
)

model_2 = build_model(
  train2_df.drop('std__value', axis = 1), 
  train2_df['std__value'], 
  settings)
model_2 = compile_model(model_2, settings)

# train the model via model.fit
history_2 = model_2.fit(
  train2_df.drop('std__value', axis = 1), 
  train2_df['std__value'], 
  epochs=settings["max_epochs"],
  batch_size=settings["batch_size"],
  shuffle=True,
  validation_data=[val2_df.drop('std__value', axis = 1), val2_df['std__value']],
  callbacks=[early_stopping_callback],
  verbose=1,
)

model_3 = build_model(
  train3_df.drop('std__value', axis = 1), 
  train3_df['std__value'], 
  settings)
model_3 = compile_model(model_3, settings)

# train the model via model.fit
history_3 = model_3.fit(
  train3_df.drop('std__value', axis = 1), 
  train3_df['std__value'], 
  epochs=settings["max_epochs"],
  batch_size=settings["batch_size"],
  shuffle=True,
  validation_data=[val3_df.drop('std__value', axis = 1), val3_df['std__value']],
  callbacks=[early_stopping_callback],
  verbose=1,
)

model_4 = build_model(
  train4_df.drop('std__value', axis = 1), 
  train4_df['std__value'], 
  settings)
model_4 = compile_model(model_4, settings)

# train the model via model.fit
history_4 = model_4.fit(
  train4_df.drop('std__value', axis = 1), 
  train4_df['std__value'], 
  epochs=settings["max_epochs"],
  batch_size=settings["batch_size"],
  shuffle=True,
  validation_data=[val4_df.drop('std__value', axis = 1), val4_df['std__value']],
  callbacks=[early_stopping_callback],
  verbose=1,
)

model_5 = build_model(
  train5_df.drop('std__value', axis = 1), 
  train5_df['std__value'], 
  settings)
model_5 = compile_model(model_5, settings)

# train the model via model.fit
history_5 = model_5.fit(
  train5_df.drop('std__value', axis = 1), 
  train5_df['std__value'], 
  epochs=settings["max_epochs"],
  batch_size=settings["batch_size"],
  shuffle=True,
  validation_data=[val5_df.drop('std__value', axis = 1), val5_df['std__value']],
  callbacks=[early_stopping_callback],
  verbose=1,
)

model_6 = build_model(
  train6_df.drop('std__value', axis = 1), 
  train6_df['std__value'], 
  settings)
model_6 = compile_model(model_6, settings)

# train the model via model.fit
history_6 = model_6.fit(
  train6_df.drop('std__value', axis = 1), 
  train6_df['std__value'], 
  epochs=settings["max_epochs"],
  batch_size=settings["batch_size"],
  shuffle=True,
  validation_data=[val6_df.drop('std__value', axis = 1), val6_df['std__value']],
  callbacks=[early_stopping_callback],
  verbose=1,
)
```

```{python build-ts}
#| echo: false
#| output: false

model_1_ts = build_model(
  train1_ts_df.drop('std__value', axis = 1), 
  train1_ts_df['std__value'], 
  settings)
model_1_ts = compile_model(model_1_ts, settings)

# train the model via model.fit
history_1_ts = model_1_ts.fit(
  train1_ts_df.drop('std__value', axis = 1), 
  train1_ts_df['std__value'], 
  epochs=settings["max_epochs"],
  batch_size=settings["batch_size"],
  shuffle=True,
  validation_data=[val1_ts_df.drop('std__value', axis = 1), val1_ts_df['std__value']],
  callbacks=[early_stopping_callback],
  verbose=1,
)

model_2_ts = build_model(
  train2_ts_df.drop('std__value', axis = 1), 
  train2_ts_df['std__value'], 
  settings)
model_2_ts = compile_model(model_2_ts, settings)

# train the model via model.fit
history_2_ts = model_2_ts.fit(
  train2_ts_df.drop('std__value', axis = 1), 
  train2_ts_df['std__value'], 
  epochs=settings["max_epochs"],
  batch_size=settings["batch_size"],
  shuffle=True,
  validation_data=[val2_ts_df.drop('std__value', axis = 1), val2_ts_df['std__value']],
  callbacks=[early_stopping_callback],
  verbose=1,
)

model_3_ts = build_model(
  train3_ts_df.drop('std__value', axis = 1), 
  train3_ts_df['std__value'], 
  settings)
model_3_ts = compile_model(model_3_ts, settings)

# train the model via model.fit
history_3_ts = model_3_ts.fit(
  train3_ts_df.drop('std__value', axis = 1), 
  train3_ts_df['std__value'], 
  epochs=settings["max_epochs"],
  batch_size=settings["batch_size"],
  shuffle=True,
  validation_data=[val3_ts_df.drop('std__value', axis = 1), val3_ts_df['std__value']],
  callbacks=[early_stopping_callback],
  verbose=1,
)


model_4_ts = build_model(
  train4_ts_df.drop('std__value', axis = 1), 
  train4_ts_df['std__value'], 
  settings)
model_4_ts = compile_model(model_4_ts, settings)

# train the model via model.fit
history_4_ts = model_4_ts.fit(
  train4_ts_df.drop('std__value', axis = 1), 
  train4_ts_df['std__value'], 
  epochs=settings["max_epochs"],
  batch_size=settings["batch_size"],
  shuffle=True,
  validation_data=[val4_ts_df.drop('std__value', axis = 1), val4_ts_df['std__value']],
  callbacks=[early_stopping_callback],
  verbose=1,
)

```

## Results

```{python train-1}
#| echo: false
fig1, axs = plt.subplots(1, 2, figsize=(10, 4))

axs[0].plot(history_1.history["loss"], label="training")
axs[0].plot(history_1.history["val_loss"], label="validation")
axs[0].set_xlabel("Epoch")
axs[0].set_ylabel("Loss")
axs[0].legend()

axs[1].plot(history_1.history["loss"], label="training")
axs[1].plot(history_1.history["val_loss"], label="validation")
axs[1].set_xlabel("Epoch")
axs[1].set_ylabel("Mean Squared Error")
axs[1].legend()

fig1.suptitle("LOO dataset 1")
```

```{python train-2}
#| echo: false
fig2, axs = plt.subplots(1, 2, figsize=(10, 4))

axs[0].plot(history_2.history["loss"], label="training")
axs[0].plot(history_2.history["val_loss"], label="validation")
axs[0].set_xlabel("Epoch")
axs[0].set_ylabel("Loss")
axs[0].legend()

axs[1].plot(history_2.history["loss"], label="training")
axs[1].plot(history_2.history["val_loss"], label="validation")
axs[1].set_xlabel("Epoch")
axs[1].set_ylabel("Mean Squared Error")
axs[1].legend()

fig2.suptitle("LOO dataset 2")
```

```{python train-3}
#| echo: false
fig3, axs = plt.subplots(1, 2, figsize=(10, 4))

axs[0].plot(history_3.history["loss"], label="training")
axs[0].plot(history_3.history["val_loss"], label="validation")
axs[0].set_xlabel("Epoch")
axs[0].set_ylabel("Loss")
axs[0].legend()

axs[1].plot(history_3.history["loss"], label="training")
axs[1].plot(history_3.history["val_loss"], label="validation")
axs[1].set_xlabel("Epoch")
axs[1].set_ylabel("Mean Squared Error")
axs[1].legend()

fig3.suptitle("LOO dataset 3")
```

```{python train-4}
#| echo: false
fig4, axs = plt.subplots(1, 2, figsize=(10, 4))

axs[0].plot(history_4.history["loss"], label="training")
axs[0].plot(history_4.history["val_loss"], label="validation")
axs[0].set_xlabel("Epoch")
axs[0].set_ylabel("Loss")
axs[0].legend()

axs[1].plot(history_4.history["loss"], label="training")
axs[1].plot(history_4.history["val_loss"], label="validation")
axs[1].set_xlabel("Epoch")
axs[1].set_ylabel("Mean Squared Error")
axs[1].legend()

fig4.suptitle("LOO dataset 4")
```

```{python train-5}
#| echo: false
fig5, axs = plt.subplots(1, 2, figsize=(10, 4))

axs[0].plot(history_5.history["loss"], label="training")
axs[0].plot(history_5.history["val_loss"], label="validation")
axs[0].set_xlabel("Epoch")
axs[0].set_ylabel("Loss")
axs[0].legend()

axs[1].plot(history_5.history["loss"], label="training")
axs[1].plot(history_5.history["val_loss"], label="validation")
axs[1].set_xlabel("Epoch")
axs[1].set_ylabel("Mean Squared Error")
axs[1].legend()

fig5.suptitle("LOO dataset 5")
```

```{python train-6}
#| echo: false
fig6, axs = plt.subplots(1, 2, figsize=(10, 4))

axs[0].plot(history_6.history["loss"], label="training")
axs[0].plot(history_6.history["val_loss"], label="validation")
axs[0].set_xlabel("Epoch")
axs[0].set_ylabel("Loss")
axs[0].legend()

axs[1].plot(history_6.history["loss"], label="training")
axs[1].plot(history_6.history["val_loss"], label="validation")
axs[1].set_xlabel("Epoch")
axs[1].set_ylabel("Mean Squared Error")
axs[1].legend()

fig6.suptitle("LOO dataset 6")
```

```{python train-ts-1}
#| echo: false
fig1ts, axs = plt.subplots(1, 2, figsize=(10, 4))

axs[0].plot(history_1.history["loss"], label="training")
axs[0].plot(history_1.history["val_loss"], label="validation")
axs[0].set_xlabel("Epoch")
axs[0].set_ylabel("Loss")
axs[0].legend()

axs[1].plot(history_1.history["loss"], label="training")
axs[1].plot(history_1.history["val_loss"], label="validation")
axs[1].set_xlabel("Epoch")
axs[1].set_ylabel("Mean Squared Error")
axs[1].legend()

fig1ts.suptitle("TS dataset 1")
```

```{python train-ts-2}
#| echo: false
fig2ts, axs = plt.subplots(1, 2, figsize=(10, 4))

axs[0].plot(history_2.history["loss"], label="training")
axs[0].plot(history_2.history["val_loss"], label="validation")
axs[0].set_xlabel("Epoch")
axs[0].set_ylabel("Loss")
axs[0].legend()

axs[1].plot(history_2.history["loss"], label="training")
axs[1].plot(history_2.history["val_loss"], label="validation")
axs[1].set_xlabel("Epoch")
axs[1].set_ylabel("Mean Squared Error")
axs[1].legend()

fig2ts.suptitle("TS dataset 2")
```

```{python train-ts-3}
#| echo: false
fig3ts, axs = plt.subplots(1, 2, figsize=(10, 4))

axs[0].plot(history_3.history["loss"], label="training")
axs[0].plot(history_3.history["val_loss"], label="validation")
axs[0].set_xlabel("Epoch")
axs[0].set_ylabel("Loss")
axs[0].legend()

axs[1].plot(history_3.history["loss"], label="training")
axs[1].plot(history_3.history["val_loss"], label="validation")
axs[1].set_xlabel("Epoch")
axs[1].set_ylabel("Mean Squared Error")
axs[1].legend()

fig3ts.suptitle("TS dataset 3")
```

```{python train-ts-4}
#| echo: false
fig4ts, axs = plt.subplots(1, 2, figsize=(10, 4))

axs[0].plot(history_4.history["loss"], label="training")
axs[0].plot(history_4.history["val_loss"], label="validation")
axs[0].set_xlabel("Epoch")
axs[0].set_ylabel("Loss")
axs[0].legend()

axs[1].plot(history_4.history["loss"], label="training")
axs[1].plot(history_4.history["val_loss"], label="validation")
axs[1].set_xlabel("Epoch")
axs[1].set_ylabel("Mean Squared Error")
axs[1].legend()

fig4ts.suptitle("TS dataset 4")
```

```{python show-val-loss}
#| echo: false
fig1.show()
fig2.show()
fig3.show()
fig4.show()
fig5.show()
fig6.show()

fig1ts.show()
fig2ts.show()
fig3ts.show()
fig4ts.show()
```

```{python loo-calc-pred}
#| echo: false
#| output: false
def calculate_vals(transformed_val, mean, std):
  actual_val = (transformed_val * std) + mean
  return actual_val

mean_1 = train1['value'].mean()
std_1 = train1['value'].std()
pred_1 = model_1.predict(train1_df.drop('std__value', axis = 1))
val_1 = model_1.predict(val1_df.drop('std__value', axis = 1))
p_act_1 = calculate_vals(pred_1, mean_1, std_1)
v_act_1 = calculate_vals(val_1, mean_1, std_1)

mean_2 = train2['value'].mean()
std_2 = train2['value'].std()
pred_2 = model_2.predict(train2_df.drop('std__value', axis = 1))
val_2 = model_2.predict(val2_df.drop('std__value', axis = 1))
p_act_2 = calculate_vals(pred_2, mean_2, std_2)
v_act_2 = calculate_vals(val_2, mean_2, std_2)

mean_3 = train3['value'].mean()
std_3 = train3['value'].std()
pred_3 = model_3.predict(train3_df.drop('std__value', axis = 1))
val_3 = model_3.predict(val3_df.drop('std__value', axis = 1))
p_act_3 = calculate_vals(pred_3, mean_3, std_3)
v_act_3 = calculate_vals(val_3, mean_3, std_3)

mean_4 = train4['value'].mean()
std_4 = train4['value'].std()
pred_4 = model_4.predict(train4_df.drop('std__value', axis = 1))
val_4 = model_4.predict(val4_df.drop('std__value', axis = 1))
p_act_4 = calculate_vals(pred_4, mean_4, std_4)
v_act_4 = calculate_vals(val_4, mean_4, std_4)

mean_5 = train5['value'].mean()
std_5 = train5['value'].std()
pred_5 = model_5.predict(train5_df.drop('std__value', axis = 1))
val_5 = model_5.predict(val5_df.drop('std__value', axis = 1))
p_act_5 = calculate_vals(pred_5, mean_5, std_5)
v_act_5 = calculate_vals(val_5, mean_5, std_5)

mean_6 = train6['value'].mean()
std_6 = train6['value'].std()
pred_6 = model_6.predict(train6_df.drop('std__value', axis = 1))
val_6 = model_6.predict(val6_df.drop('std__value', axis = 1))
p_act_6 = calculate_vals(pred_6, mean_6, std_6)
v_act_6 = calculate_vals(val_6, mean_6, std_6)

```

```{python ts-calc-pred}
#| echo: false
#| output: false

mean_1_ts = train1_ts['value'].mean()
std_1_ts = train1_ts['value'].std()
pred_1_ts = model_1_ts.predict(train1_ts_df.drop('std__value', axis = 1))
val_1_ts = model_1_ts.predict(val1_ts_df.drop('std__value', axis = 1))
p_act_1_ts = calculate_vals(pred_1_ts, mean_1_ts, std_1_ts)
v_act_1_ts = calculate_vals(val_1_ts, mean_1_ts, std_1_ts)

mean_2_ts = train2_ts['value'].mean()
std_2_ts = train2_ts['value'].std()
pred_2_ts = model_2_ts.predict(train2_ts_df.drop('std__value', axis = 1))
val_2_ts = model_2_ts.predict(val2_ts_df.drop('std__value', axis = 1))
p_act_2_ts = calculate_vals(pred_2_ts, mean_2_ts, std_2_ts)
v_act_2_ts = calculate_vals(val_2_ts, mean_2_ts, std_2_ts)

mean_3_ts = train3_ts['value'].mean()
std_3_ts = train3_ts['value'].std()
pred_3_ts = model_3_ts.predict(train3_ts_df.drop('std__value', axis = 1))
val_3_ts = model_3_ts.predict(val3_ts_df.drop('std__value', axis = 1))
p_act_3_ts = calculate_vals(pred_3_ts, mean_3_ts, std_3_ts)
v_act_3_ts = calculate_vals(val_3_ts, mean_3_ts, std_3_ts)

mean_4_ts = train4_ts['value'].mean()
std_4_ts = train4_ts['value'].std()
pred_4_ts = model_4_ts.predict(train4_ts_df.drop('std__value', axis = 1))
val_4_ts = model_4_ts.predict(val4_ts_df.drop('std__value', axis = 1))
p_act_4_ts = calculate_vals(pred_4_ts, mean_4_ts, std_4_ts)
v_act_4_ts = calculate_vals(val_4_ts, mean_4_ts, std_4_ts)

```

```{r assign-pred, echo=FALSE}
train1$pred = py$p_act_1
train2$pred = py$p_act_2
train3$pred = py$p_act_3
train4$pred = py$p_act_4
train5$pred = py$p_act_5
train6$pred = py$p_act_6

val1$pred = py$v_act_1
val2$pred = py$v_act_2
val3$pred = py$v_act_3
val4$pred = py$v_act_4
val5$pred = py$v_act_5
val6$pred = py$v_act_6

train1_ts$pred = py$p_act_1_ts
train2_ts$pred = py$p_act_2_ts
train3_ts$pred = py$p_act_3_ts
train4_ts$pred = py$p_act_4_ts

val1_ts$pred = py$v_act_1_ts
val2_ts$pred = py$v_act_2_ts
val3_ts$pred = py$v_act_3_ts
val4_ts$pred = py$v_act_4_ts

```

### Leave one out validations

These are super hit-or-miss. MSE ranges from 3-10. Pretty terrible.

```{r loo-val-figs, echo=F}
ggplot(val1, aes(x = date, y = value)) +
  geom_point() +
  geom_point(aes(y = pred), color = 'orange') +
  facet_grid(feature ~ .) +
  theme_bw()

ggplot(val2, aes(x = date, y = value)) +
  geom_point() +
  geom_point(aes(y = pred), color = 'orange') +
  facet_grid(feature ~ .) +
  theme_bw()

ggplot(val3, aes(x = date, y = value)) +
  geom_point() +
  geom_point(aes(y = pred), color = 'orange') +
  facet_grid(feature ~ .) +
  theme_bw()

ggplot(val4, aes(x = date, y = value)) +
  geom_point() +
  geom_point(aes(y = pred), color = 'orange') +
  facet_grid(feature ~ .) +
  theme_bw()

ggplot(val5, aes(x = date, y = value)) +
  geom_point() +
  geom_point(aes(y = pred), color = 'orange') +
  facet_grid(feature ~ .) +
  theme_bw()

ggplot(val6, aes(x = date, y = value)) +
  geom_point() +
  geom_point(aes(y = pred), color = 'orange') +
  facet_grid(feature ~ .) +
  theme_bw()

```

### Timeseries split validations

These are good until the most recent data, which look horrid. MSE is smaller, but still higher than baseline (3-5).

```{r ts-val-figs, echo = F}
ggplot(val1_ts, aes(x = date, y = value)) +
  geom_point() +
  geom_point(aes(y = pred), color = 'orange') +
  facet_grid(feature ~ .) +
  theme_bw()

ggplot(val2_ts, aes(x = date, y = value)) +
  geom_point() +
  geom_point(aes(y = pred), color = 'orange') +
  facet_grid(feature ~ .) +
  theme_bw()

ggplot(val3_ts, aes(x = date, y = value)) +
  geom_point() +
  geom_point(aes(y = pred), color = 'orange') +
  facet_grid(feature ~ .) +
  theme_bw()

ggplot(val4_ts, aes(x = date, y = value)) +
  geom_point() +
  geom_point(aes(y = pred), color = 'orange') +
  facet_grid(feature ~ .) +
  theme_bw()
```

```{r, echo = F, eval = F}
Metrics::mse(val1$value, val1$pred)
Metrics::mse(train1$value, train1$pred)

Metrics::mse(val2$value, val2$pred)
Metrics::mse(train2$value, train2$pred)

Metrics::mse(val3$value, val3$pred)
Metrics::mse(train3$value, train3$pred)

Metrics::mse(val4$value, val4$pred)
Metrics::mse(train4$value, train4$pred)

Metrics::mse(val5$value, val5$pred)
Metrics::mse(train5$value, train5$pred)

Metrics::mse(val6$value, val6$pred)
Metrics::mse(train6$value, train6$pred)

Metrics::mse(val1_ts$value, val1_ts$pred)
Metrics::mse(train1_ts$value, train1_ts$pred)

Metrics::mse(val2_ts$value, val2_ts$pred)
Metrics::mse(train2_ts$value, train2_ts$pred)

Metrics::mse(val3_ts$value, val3_ts$pred)
Metrics::mse(train3_ts$value, train3_ts$pred)

Metrics::mse(val4_ts$value, val4_ts$pred)
Metrics::mse(train4_ts$value, train4_ts$pred)
```

### Hyper-parameter tuning

Current settings:

```{python}
settings = {
    "hiddens": [3, 3],
    "activations": ["relu", "relu"],
    "learning_rate": 0.001,
    "random_seed": 57,
    "max_epochs": 1000,
    "batch_size": 32,
    "patience": 10,
    "dropout_rate": 0,
}
```

<!-- ### Hindcasting application -->

<!-- ### Discussion -->

<!-- ## Supporting Figures -->
