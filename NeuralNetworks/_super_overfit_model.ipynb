{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script creates a very overfit model for predicting surface temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#high level modules\n",
    "import os\n",
    "import sys\n",
    "import imp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# ml/ai modules\n",
    "import tensorflow as tf\n",
    "# Let's import some different things we will use to build the neural network\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout, Softmax\n",
    "\n",
    "# import custom modules\n",
    "this_dir = \"/Users/steeleb/Documents/GitHub/ATS-ML-Fall2023/\"\n",
    "imp.load_source(\"settings\",os.path.join(this_dir,\"NeuralNetworks/settings.py\"))\n",
    "from settings import settings\n",
    "imp.load_source(\"tvt\", os.path.join(this_dir, \"preprocessing.py\"))\n",
    "from tvt import train1, val1, train2, val2, train3, val3, train4, val4, train5, val5, train6, val6\n",
    "from tvt import train1_ts, val1_ts, train2_ts, val2_ts, train3_ts, val3_ts, train4_ts, val4_ts\n",
    "imp.load_source(\"architecture\", os.path.join(this_dir, \"NeuralNetworks/architecture.py\"))\n",
    "from architecture import build_model, compile_model\n",
    "imp.load_source(\"universals\", os.path.join(this_dir, \"universal_functions.py\"))\n",
    "from universals import save_to_pickle, get_features_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format training and validation arrays for use in model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features1, labels_1, val_features1, val_labels_1 = get_features_labels(train1, val1)\n",
    "features2, labels_2, val_features2, val_labels_2 = get_features_labels(train2, val2)\n",
    "features3, labels_3, val_features3, val_labels_3 = get_features_labels(train3, val3)\n",
    "features4, labels_4, val_features4, val_labels_4 = get_features_labels(train4, val4)\n",
    "features5, labels_5, val_features5, val_labels_5 = get_features_labels(train5, val5)\n",
    "features6, labels_6, val_features6, val_labels_6 = get_features_labels(train6, val6)\n",
    "\n",
    "ts_features1, ts_labels_1, ts_val_features1, ts_val_labels_1 = get_features_labels(train1_ts, val1_ts)\n",
    "ts_features2, ts_labels_2, ts_val_features2, ts_val_labels_2 = get_features_labels(train2_ts, val2_ts)\n",
    "ts_features3, ts_labels_3, ts_val_features3, ts_val_labels_3 = get_features_labels(train3_ts, val3_ts)\n",
    "ts_features4, ts_labels_4, ts_val_features4, ts_val_labels_4 = get_features_labels(train4_ts, val4_ts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile and train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp.reload(sys.modules['settings'])\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "tf.keras.utils.set_random_seed(settings[\"super_overfit\"][\"random_seed\"])\n",
    "\n",
    "# define the early stopping callback\n",
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "  monitor=\"val_loss\", \n",
    "  patience=settings[\"super_overfit\"][\"patience\"], \n",
    "  restore_best_weights=True, \n",
    "  mode=\"auto\"\n",
    ")\n",
    "\n",
    "## LOO 1\n",
    "model_1 = build_model(\n",
    "  features1, \n",
    "  labels_1, \n",
    "  settings[\"super_overfit\"])\n",
    "\n",
    "model_1 = compile_model(\n",
    "  model_1, \n",
    "  settings['super_overfit'])\n",
    "\n",
    "# train the model via model.fit\n",
    "history_1 = model_1.fit(\n",
    "  features1, \n",
    "  labels_1, \n",
    "  epochs=settings['super_overfit'][\"max_epochs\"],\n",
    "  batch_size=settings['super_overfit'][\"batch_size\"],\n",
    "  shuffle=True,\n",
    "  validation_data=[val_features1, val_labels_1],\n",
    "  callbacks=[early_stopping_callback],\n",
    "  verbose=1,\n",
    ")\n",
    "\n",
    "## LOO 2\n",
    "model_2 = build_model(\n",
    "  features2,\n",
    "  labels_2, \n",
    "  settings[\"super_overfit\"])\n",
    "model_2 = compile_model(model_2, settings[\"super_overfit\"])\n",
    "\n",
    "# train the model via model.fit\n",
    "history_2 = model_2.fit(\n",
    "  features2,\n",
    "  labels_2,\n",
    "  epochs=settings[\"super_overfit\"][\"max_epochs\"],\n",
    "  batch_size=settings[\"super_overfit\"][\"batch_size\"],\n",
    "  shuffle=True,\n",
    "  validation_data=[val_features2, val_labels_2],\n",
    "  callbacks=[early_stopping_callback],\n",
    "  verbose=1,\n",
    ")\n",
    "\n",
    "## LOO 3\n",
    "\n",
    "model_3 = build_model(\n",
    "  features3,\n",
    "  labels_3,\n",
    "  settings[\"super_overfit\"])\n",
    "model_3 = compile_model(model_3, settings[\"super_overfit\"])\n",
    "\n",
    "# train the model via model.fit\n",
    "history_3 = model_3.fit(\n",
    "  features3,\n",
    "  labels_3,\n",
    "  epochs=settings[\"super_overfit\"][\"max_epochs\"],\n",
    "  batch_size=settings[\"super_overfit\"][\"batch_size\"],\n",
    "  shuffle=True,\n",
    "  validation_data=[val_features3, val_labels_3],\n",
    "  callbacks=[early_stopping_callback],\n",
    "  verbose=1,\n",
    ")\n",
    "\n",
    "## LOO 4\n",
    "\n",
    "model_4 = build_model(\n",
    "  features4,\n",
    "  labels_4,\n",
    "  settings[\"super_overfit\"])\n",
    "model_4 = compile_model(model_4, settings[\"super_overfit\"])\n",
    "\n",
    "# train the model via model.fit\n",
    "history_4 = model_4.fit(\n",
    "  features4,\n",
    "  labels_4,\n",
    "  epochs=settings[\"super_overfit\"][\"max_epochs\"],\n",
    "  batch_size=settings[\"super_overfit\"][\"batch_size\"],\n",
    "  shuffle=True,\n",
    "  validation_data=[val_features4, val_labels_4],\n",
    "  callbacks=[early_stopping_callback],\n",
    "  verbose=1,\n",
    ")\n",
    "\n",
    "## LOO 5\n",
    "\n",
    "model_5 = build_model(\n",
    "  features5,\n",
    "  labels_5,\n",
    "  settings[\"super_overfit\"])\n",
    "model_5 = compile_model(model_5, settings[\"super_overfit\"])\n",
    "\n",
    "# train the model via model.fit\n",
    "history_5 = model_5.fit(\n",
    "  features5,\n",
    "  labels_5,\n",
    "  epochs=settings[\"super_overfit\"][\"max_epochs\"],\n",
    "  batch_size=settings[\"super_overfit\"][\"batch_size\"],\n",
    "  shuffle=True,\n",
    "  validation_data=[val_features5, val_labels_5],\n",
    "  callbacks=[early_stopping_callback],\n",
    "  verbose=1,\n",
    ")\n",
    "\n",
    "## LOO 6\n",
    "\n",
    "model_6 = build_model(\n",
    "  features6,\n",
    "  labels_6,\n",
    "  settings[\"super_overfit\"])\n",
    "model_6 = compile_model(model_6, settings[\"super_overfit\"])\n",
    "\n",
    "# train the model via model.fit\n",
    "history_6 = model_6.fit(\n",
    "  features6,\n",
    "  labels_6,\n",
    "  epochs=settings[\"super_overfit\"][\"max_epochs\"],\n",
    "  batch_size=settings[\"super_overfit\"][\"batch_size\"],\n",
    "  shuffle=True,\n",
    "  validation_data=[val_features6, val_labels_6],\n",
    "  callbacks=[early_stopping_callback],\n",
    "  verbose=1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And save the models and training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_dir = \"/Users/steeleb/OneDrive - Colostate/NASA-Northern/data/NN_train_val_test/models/super_overfit/\"\n",
    "\n",
    "# save models to pickle\n",
    "models = [model_1, model_2, model_3, model_4, model_5, model_6]\n",
    "\n",
    "for model, i in zip(models, range(1,7)):\n",
    "    save_to_pickle(model, f\"{dump_dir}/model_{i}.pkl\")\n",
    "\n",
    "# save history to pickles\n",
    "histories = [history_1, history_2, history_3, history_4, history_5, history_6]\n",
    "\n",
    "for history, i in zip(histories, range(1,7)):\n",
    "    save_to_pickle(history, f\"{dump_dir}/history_{i}.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then do the same for timeseries train/val sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "tf.keras.utils.set_random_seed(settings[\"basic\"][\"random_seed\"])\n",
    "\n",
    "# define the early stopping callback\n",
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "  monitor=\"val_loss\", \n",
    "  patience=settings[\"basic\"][\"patience\"], \n",
    "  restore_best_weights=True, \n",
    "  mode=\"auto\"\n",
    ")\n",
    "\n",
    "## TS 1\n",
    "model_1_ts = build_model(\n",
    "  ts_features1, \n",
    "  ts_labels_1, \n",
    "  settings[\"basic\"])\n",
    "\n",
    "model_1_ts = compile_model(\n",
    "  model_1_ts, \n",
    "  settings['basic'])\n",
    "\n",
    "# train the model via model.fit\n",
    "history_1_ts = model_1_ts.fit(\n",
    "  ts_features1, \n",
    "  ts_labels_1, \n",
    "  epochs=settings['basic'][\"max_epochs\"],\n",
    "  batch_size=settings['basic'][\"batch_size\"],\n",
    "  shuffle=True,\n",
    "  validation_data=[ts_val_features1, ts_val_labels_1],\n",
    "  callbacks=[early_stopping_callback],\n",
    "  verbose=1,\n",
    ")\n",
    "\n",
    "## TS 2\n",
    "model_2_ts = build_model(\n",
    "  ts_features2,\n",
    "  ts_labels_2, \n",
    "  settings[\"basic\"])\n",
    "model_2_ts = compile_model(model_2_ts, settings[\"basic\"])\n",
    "\n",
    "# train the model via model.fit\n",
    "history_2_ts = model_2_ts.fit(\n",
    "  ts_features2,\n",
    "  ts_labels_2,\n",
    "  epochs=settings[\"basic\"][\"max_epochs\"],\n",
    "  batch_size=settings[\"basic\"][\"batch_size\"],\n",
    "  shuffle=True,\n",
    "  validation_data=[ts_val_features2, ts_val_labels_2],\n",
    "  callbacks=[early_stopping_callback],\n",
    "  verbose=1,\n",
    ")\n",
    "\n",
    "## TS 3\n",
    "\n",
    "model_3_ts = build_model(\n",
    "  ts_features3,\n",
    "  ts_labels_3,\n",
    "  settings[\"basic\"])\n",
    "model_3_ts = compile_model(model_3_ts, settings[\"basic\"])\n",
    "\n",
    "# train the model via model.fit\n",
    "history_3_ts = model_3_ts.fit(\n",
    "  ts_features3,\n",
    "  ts_labels_3,\n",
    "  epochs=settings[\"basic\"][\"max_epochs\"],\n",
    "  batch_size=settings[\"basic\"][\"batch_size\"],\n",
    "  shuffle=True,\n",
    "  validation_data=[ts_val_features3, ts_val_labels_3],\n",
    "  callbacks=[early_stopping_callback],\n",
    "  verbose=1,\n",
    ")\n",
    "\n",
    "## TS 4\n",
    "\n",
    "model_4_ts = build_model(\n",
    "  ts_features4,\n",
    "  ts_labels_4,\n",
    "  settings[\"basic\"])\n",
    "model_4_ts = compile_model(model_4_ts, settings[\"basic\"])\n",
    "\n",
    "# train the model via model.fit\n",
    "history_4_ts = model_4_ts.fit(\n",
    "  ts_features4,\n",
    "  ts_labels_4,\n",
    "  epochs=settings[\"basic\"][\"max_epochs\"],\n",
    "  batch_size=settings[\"basic\"][\"batch_size\"],\n",
    "  shuffle=True,\n",
    "  validation_data=[ts_val_features4, ts_val_labels_4],\n",
    "  callbacks=[early_stopping_callback],\n",
    "  verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And save to pickles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save models to pickle\n",
    "ts_models = [model_1_ts, model_2_ts, model_3_ts, model_4_ts]\n",
    "\n",
    "for model, i in zip(ts_models, range(1,7)):\n",
    "    save_to_pickle(model, f\"{dump_dir}/ts_model_{i}.pkl\")\n",
    "\n",
    "# save history to pickles\n",
    "ts_histories = [history_1_ts, history_2_ts, history_3_ts, history_4_ts]\n",
    "\n",
    "for history, i in zip(ts_histories, range(1,7)):\n",
    "    save_to_pickle(history, f\"{dump_dir}/ts_history_{i}.pkl\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_ATSML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
